{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:18:19.319135Z",
     "iopub.status.busy": "2025-11-01T16:18:19.318462Z",
     "iopub.status.idle": "2025-11-01T16:18:22.947899Z",
     "shell.execute_reply": "2025-11-01T16:18:22.947083Z",
     "shell.execute_reply.started": "2025-11-01T16:18:19.319112Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle test run successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Kaggle test run successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:18:28.569400Z",
     "iopub.status.busy": "2025-11-01T16:18:28.568724Z",
     "iopub.status.idle": "2025-11-01T16:19:31.814430Z",
     "shell.execute_reply": "2025-11-01T16:19:31.813691Z",
     "shell.execute_reply.started": "2025-11-01T16:18:28.569375Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm==0.4.12\n",
      "  Downloading timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.11/dist-packages (from timm==0.4.12) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm==0.4.12) (0.21.0+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.12) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.12) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.12) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.12) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.12) (2025.9.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.4->timm==0.4.12)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.4->timm==0.4.12)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.4->timm==0.4.12)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.4->timm==0.4.12)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.4->timm==0.4.12)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.4->timm==0.4.12)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.4->timm==0.4.12)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.4->timm==0.4.12)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.4->timm==0.4.12)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.12) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.12) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.12) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.4->timm==0.4.12)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.12) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.12) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4->timm==0.4.12) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm==0.4.12) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm==0.4.12) (11.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.4->timm==0.4.12) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm==0.4.12) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm==0.4.12) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm==0.4.12) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm==0.4.12) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm==0.4.12) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm==0.4.12) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->timm==0.4.12) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->timm==0.4.12) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->timm==0.4.12) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->timm==0.4.12) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->timm==0.4.12) (2024.2.0)\n",
      "Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m162.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m187.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m167.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m183.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m179.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m162.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m148.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m134.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m122.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "  Attempting uninstall: timm\n",
      "    Found existing installation: timm 1.0.19\n",
      "    Uninstalling timm-1.0.19:\n",
      "      Successfully uninstalled timm-1.0.19\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 timm-0.4.12\n"
     ]
    }
   ],
   "source": [
    "!pip install timm==0.4.12 --no-cache-dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:20:00.780183Z",
     "iopub.status.busy": "2025-11-01T16:20:00.779397Z",
     "iopub.status.idle": "2025-11-01T16:20:04.468671Z",
     "shell.execute_reply": "2025-11-01T16:20:04.467678Z",
     "shell.execute_reply.started": "2025-11-01T16:20:00.780151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install continuum>=1.0.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:24:16.742188Z",
     "iopub.status.busy": "2025-11-01T16:24:16.741427Z",
     "iopub.status.idle": "2025-11-01T16:24:18.211662Z",
     "shell.execute_reply": "2025-11-01T16:24:18.211062Z",
     "shell.execute_reply.started": "2025-11-01T16:24:16.742163Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CDDBBinaryDataset(Dataset):\n",
    "    def __init__(self, root, task, split, transform):\n",
    "        self.root = root\n",
    "        self.task = task\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.task_id = task_id\n",
    "\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        self.task_ids = []\n",
    "\n",
    "        # Path setup\n",
    "        task_folder = os.path.join(root, task)\n",
    "        nested_folder = os.path.join(task_folder, task)\n",
    "        if os.path.exists(nested_folder):\n",
    "            task_folder = nested_folder\n",
    "\n",
    "        task_folder = os.path.join(task_folder, split)\n",
    "        if not os.path.exists(task_folder):\n",
    "            print(f\"[Warning] Task folder not found: {task_folder}\")\n",
    "            return\n",
    "\n",
    "        # Load images\n",
    "        if task.lower() == \"cyclegan\":\n",
    "            for subfolder in os.listdir(task_folder):\n",
    "                subfolder_path = os.path.join(task_folder, subfolder)\n",
    "                if not os.path.isdir(subfolder_path):\n",
    "                    continue\n",
    "                for sub, label in [(\"0_real\", 0), (\"1_fake\", 1)]:\n",
    "                    label_folder = os.path.join(subfolder_path, sub)\n",
    "                    if not os.path.exists(label_folder):\n",
    "                        continue\n",
    "                    for img_name in os.listdir(label_folder):\n",
    "                        if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                            self.data.append(os.path.join(label_folder, img_name))\n",
    "                            self.targets.append(label)\n",
    "                            self.task_ids.append(task_id)\n",
    "        else:\n",
    "            for sub, label in [(\"0_real\", 0), (\"1_fake\", 1)]:\n",
    "                label_folder = os.path.join(task_folder, sub)\n",
    "                if not os.path.exists(label_folder):\n",
    "                    continue\n",
    "                for img_name in os.listdir(label_folder):\n",
    "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        self.data.append(os.path.join(label_folder, img_name))\n",
    "                        self.targets.append(label)\n",
    "                        self.task_ids.append(task_id)\n",
    "\n",
    "        # For compatibility with Memory class we'll expose these attributes\n",
    "        # as arrays/lists. Memory may replace these later.\n",
    "        self._x = list(self.data)\n",
    "        self._y = list(self.targets)\n",
    "        self._t = list(self.task_ids)\n",
    "\n",
    "        print(f\"[INFO] Loaded {len(self._x)} images for task '{task}' ({split})\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns (image_tensor, label, task_id)\n",
    "        - This method uses self._x/_y/_t if present, otherwise falls back to internal lists.\n",
    "        - If self._x contains image *paths*, it opens them on the fly (lazy load).\n",
    "        - If self._x contains preloaded tensors/arrays, returns them directly.\n",
    "        \"\"\"\n",
    "        # Prefer _x/_y/_t (these may be set by Memory.get_dataset)\n",
    "        if hasattr(self, '_x') and self._x is not None:\n",
    "            x = self._x[idx]\n",
    "            y = self._y[idx]\n",
    "            t = self._t[idx]\n",
    "        else:\n",
    "            x = self.data[idx]\n",
    "            y = self.targets[idx]\n",
    "            t = self.task_ids[idx]\n",
    "\n",
    "        # If x is a path-like string, load image\n",
    "        if isinstance(x, str) or (hasattr(x, 'endswith') and callable(x.endswith)):\n",
    "            img = Image.open(x).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "        else:\n",
    "            # assume tensor/ndarray already\n",
    "            img = x\n",
    "            if self.transform and not isinstance(img, torch.Tensor):\n",
    "                # usually transform expects PIL, so skip transform here if already tensor\n",
    "                pass\n",
    "\n",
    "        return img, y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:24:26.087303Z",
     "iopub.status.busy": "2025-11-01T16:24:26.086619Z",
     "iopub.status.idle": "2025-11-01T16:24:27.277135Z",
     "shell.execute_reply": "2025-11-01T16:24:27.276484Z",
     "shell.execute_reply.started": "2025-11-01T16:24:26.087277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "   transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:24:31.477028Z",
     "iopub.status.busy": "2025-11-01T16:24:31.476188Z",
     "iopub.status.idle": "2025-11-01T16:24:31.587451Z",
     "shell.execute_reply": "2025-11-01T16:24:31.586610Z",
     "shell.execute_reply.started": "2025-11-01T16:24:31.476999Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 6000 images for task 'gaugan' (train)\n",
      "[INFO] Loaded 2000 images for task 'gaugan' (val)\n",
      "[INFO] Loaded 2400 images for task 'biggan' (train)\n",
      "[INFO] Loaded 800 images for task 'biggan' (val)\n",
      "[INFO] Loaded 1572 images for task 'cyclegan' (train)\n",
      "[INFO] Loaded 524 images for task 'cyclegan' (val)\n",
      "[INFO] Loaded 7656 images for task 'imle' (train)\n",
      "[INFO] Loaded 2552 images for task 'imle' (val)\n",
      "[INFO] Loaded 3248 images for task 'deepfake' (train)\n",
      "[INFO] Loaded 1082 images for task 'deepfake' (val)\n",
      "[INFO] Loaded 7658 images for task 'crn' (train)\n",
      "[INFO] Loaded 2552 images for task 'crn' (val)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "tasks = [\"gaugan\",\"biggan\",\"cyclegan\",\"imle\",\"deepfake\",\"crn\"]  # EASY sequence\n",
    "root = \"/kaggle/input/cddb-easy\"\n",
    "\n",
    "task_loaders = []\n",
    "val_loaders = []\n",
    "for task_id, task in enumerate(tasks):\n",
    "    \n",
    "    train_dataset = CDDBBinaryDataset(root=root, task=task, split=\"train\", transform=train_transform)\n",
    "    val_dataset = CDDBBinaryDataset(root=root, task=task, split=\"val\", transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    task_loaders.append(train_loader)\n",
    "    val_loaders.append(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:20:36.652162Z",
     "iopub.status.busy": "2025-11-01T16:20:36.651838Z",
     "iopub.status.idle": "2025-11-01T16:20:36.657154Z",
     "shell.execute_reply": "2025-11-01T16:20:36.656291Z",
     "shell.execute_reply.started": "2025-11-01T16:20:36.652141Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created folder: ['.virtual_documents', '=1.0.27', 'continual']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create your custom folder\n",
    "os.makedirs(\"/kaggle/working/continual\", exist_ok=True)\n",
    "print(\"Created folder:\", os.listdir(\"/kaggle/working\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:20:39.123047Z",
     "iopub.status.busy": "2025-11-01T16:20:39.122750Z",
     "iopub.status.idle": "2025-11-01T16:20:39.132152Z",
     "shell.execute_reply": "2025-11-01T16:20:39.131231Z",
     "shell.execute_reply.started": "2025-11-01T16:20:39.123024Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created!\n"
     ]
    }
   ],
   "source": [
    "# Write Python code to a new file\n",
    "code = \"\"\"\n",
    "\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "class SmoothedValue(object):\n",
    "  \n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "       \n",
    "        if not is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if v is None:\n",
    "                continue\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def update_dict(self, d):\n",
    "        for k, v in d.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        log_msg = [\n",
    "            header,\n",
    "            '[{0' + space_fmt + '}/{1}]',\n",
    "            'eta: {eta}',\n",
    "            '{meters}',\n",
    "            'time: {time}',\n",
    "            'data: {data}'\n",
    "        ]\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg.append('max mem: {memory:.0f}')\n",
    "        log_msg = self.delimiter.join(log_msg)\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))\n",
    "\n",
    "\n",
    "def _load_checkpoint_for_ema(model_ema, checkpoint):\n",
    "    \n",
    "    mem_file = io.BytesIO()\n",
    "    torch.save(checkpoint, mem_file)\n",
    "    mem_file.seek(0)\n",
    "    model_ema._load_checkpoint(mem_file)\n",
    "\n",
    "\n",
    "def setup_for_distributed(is_master):\n",
    " \n",
    "    import builtins as __builtin__\n",
    "    builtin_print = __builtin__.print\n",
    "\n",
    "    def print(*args, **kwargs):\n",
    "        force = kwargs.pop('force', False)\n",
    "        if is_master or force:\n",
    "            builtin_print(*args, **kwargs)\n",
    "\n",
    "    __builtin__.print = print\n",
    "\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()\n",
    "\n",
    "\n",
    "def get_rank():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 0\n",
    "    return dist.get_rank()\n",
    "\n",
    "\n",
    "def is_main_process():\n",
    "    return get_rank() == 0\n",
    "\n",
    "\n",
    "def save_on_master(*args, **kwargs):\n",
    "    if is_main_process():\n",
    "        torch.save(*args, **kwargs)\n",
    "\n",
    "\n",
    "def init_distributed_mode(args):\n",
    "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
    "        args.rank = int(os.environ[\"RANK\"])\n",
    "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
    "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
    "    #elif 'SLURM_PROCID' in os.environ:\n",
    "    #    args.rank = int(os.environ['SLURM_PROCID'])\n",
    "    #    args.gpu = args.rank % torch.cuda.device_count()\n",
    "    else:\n",
    "        print('Not using distributed mode')\n",
    "        args.distributed = False\n",
    "        return\n",
    "\n",
    "    args.distributed = True\n",
    "\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    args.dist_backend = 'nccl'\n",
    "    print('| distributed init (rank {}): {}'.format(\n",
    "        args.rank, args.dist_url), flush=True)\n",
    "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                         world_size=args.world_size, rank=args.rank,\n",
    "                                         timeout=datetime.timedelta(hours=2))\n",
    "    torch.distributed.barrier()\n",
    "    setup_for_distributed(args.rank == 0)\n",
    "\n",
    "\n",
    "def load_first_task_model(model_without_ddp, loss_scaler, task_id, args):\n",
    "    strict = False\n",
    "\n",
    "    if args.resume.startswith('https'):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            args.resume, map_location='cpu', check_hash=True)\n",
    "    elif os.path.isdir(args.resume):\n",
    "        path = os.path.join(args.resume, f\"checkpoint_{task_id}.pth\")\n",
    "        checkpoint = torch.load(path, map_location='cpu')\n",
    "    else:\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "\n",
    "    model_ckpt = checkpoint['model']\n",
    "\n",
    "    if not strict:\n",
    "        for i in range(1, 6):\n",
    "            k = f\"head.fcs.{i}.weight\"\n",
    "            if k in model_ckpt: del model_ckpt[k]\n",
    "            k = f\"head.fcs.{i}.bias\"\n",
    "            if k in model_ckpt: del model_ckpt[k]\n",
    "    model_without_ddp.load_state_dict(model_ckpt, strict=strict)\n",
    "    if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
    "        #optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        #lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        #args.start_epoch = checkpoint['epoch'] + 1\n",
    "        #if args.model_ema:\n",
    "        #    utils._load_checkpoint_for_ema(model_ema, checkpoint['model_ema'])\n",
    "        if 'scaler' in checkpoint:\n",
    "            try:\n",
    "                loss_scaler.load_state_dict(checkpoint['scaler'])\n",
    "            except:\n",
    "                warnings.warn(\"Could not reload loss scaler, probably because of amp/noamp mismatch\")\n",
    "\n",
    "\n",
    "\n",
    "def change_pos_embed_size(pos_embed, new_size=32, patch_size=16, old_size=224):\n",
    "    nb_patches = (new_size // patch_size) ** 2\n",
    "    new_pos_embed = torch.randn(1, nb_patches + 1, pos_embed.shape[2])\n",
    "    new_pos_embed[0, 0] = pos_embed[0, 0]\n",
    "\n",
    "    lo_idx = 1\n",
    "    for i in range(nb_patches):\n",
    "        hi_idx = lo_idx + old_size // nb_patches\n",
    "        new_pos_embed[0, i] = pos_embed[0, lo_idx:hi_idx].mean(dim=0)\n",
    "        lo_idx = hi_idx\n",
    "\n",
    "    return torch.nn.Parameter(new_pos_embed)\n",
    "\n",
    "def freeze_parameters(m, requires_grad=False):\n",
    "    if m is None:\n",
    "        return\n",
    "\n",
    "    if isinstance(m, nn.Parameter):\n",
    "        m.requires_grad = requires_grad\n",
    "    else:\n",
    "        for p in m.parameters():\n",
    "            p.requires_grad = requires_grad\n",
    "\"\"\"\n",
    "\n",
    "with open(\"/kaggle/working/continual/utils.py\", \"w\") as f:\n",
    "    f.write(code)\n",
    "\n",
    "print(\"File created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:25:03.211987Z",
     "iopub.status.busy": "2025-11-01T16:25:03.211429Z",
     "iopub.status.idle": "2025-11-01T16:25:03.455123Z",
     "shell.execute_reply": "2025-11-01T16:25:03.454063Z",
     "shell.execute_reply.started": "2025-11-01T16:25:03.211964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "from functools import lru_cache\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "\n",
    "import continual.utils as cutils\n",
    "\n",
    "\n",
    "class BatchEnsemble(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "\n",
    "        self.out_features, self.in_features = out_features, in_features\n",
    "        self.bias = bias\n",
    "\n",
    "        self.r = nn.Parameter(torch.randn(self.out_features))\n",
    "        self.s = nn.Parameter(torch.randn(self.in_features))\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls, self.in_features, self.out_features, self.bias)\n",
    "        memo[id(self)] = result\n",
    "        for k, v in self.__dict__.items():\n",
    "            setattr(result, k, copy.deepcopy(v, memo))\n",
    "\n",
    "        result.linear.weight = self.linear.weight\n",
    "        return result\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        device = self.linear.weight.device\n",
    "        self.r = nn.Parameter(torch.randn(self.out_features).to(device))\n",
    "        self.s = nn.Parameter(torch.randn(self.in_features).to(device))\n",
    "\n",
    "        if self.bias:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.linear.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            nn.init.uniform_(self.linear.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = torch.outer(self.r, self.s)\n",
    "        w = w * self.linear.weight\n",
    "        return F.linear(x, w, self.linear.bias)\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0., fc=nn.Linear):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = fc(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = fc(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, BatchEnsemble):\n",
    "            trunc_normal_(m.linear.weight, std=.02)\n",
    "            if isinstance(m.linear, nn.Linear) and m.linear.bias is not None:\n",
    "                nn.init.constant_(m.linear.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPSA(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.,\n",
    "                 locality_strength=1., use_local_init=True, fc=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dim = dim\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qk = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
    "        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.pos_proj = nn.Linear(3, num_heads)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.locality_strength = locality_strength\n",
    "        self.gating_param = nn.Parameter(torch.ones(self.num_heads))\n",
    "        self.apply(self._init_weights)\n",
    "        if use_local_init:\n",
    "            self.local_init(locality_strength=locality_strength)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        if not hasattr(self, 'rel_indices') or self.rel_indices.size(1)!=N:\n",
    "            self.get_rel_indices(N)\n",
    "\n",
    "        attn = self.get_attention(x)\n",
    "        v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn, v\n",
    "\n",
    "    def get_attention(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qk = self.qk(x).reshape(B, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k = qk[0], qk[1]\n",
    "        pos_score = self.rel_indices.expand(B, -1, -1,-1)\n",
    "        pos_score = self.pos_proj(pos_score).permute(0,3,1,2)\n",
    "        patch_score = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        patch_score = patch_score.softmax(dim=-1)\n",
    "        pos_score = pos_score.softmax(dim=-1)\n",
    "\n",
    "        gating = self.gating_param.view(1,-1,1,1)\n",
    "        attn = (1.-torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score\n",
    "        attn /= attn.sum(dim=-1).unsqueeze(-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        return attn\n",
    "\n",
    "    def get_attention_map(self, x, return_map = False):\n",
    "\n",
    "        attn_map = self.get_attention(x).mean(0) # average over batch\n",
    "        distances = self.rel_indices.squeeze()[:,:,-1]**.5\n",
    "        dist = torch.einsum('nm,hnm->h', (distances, attn_map))\n",
    "        dist /= distances.size(0)\n",
    "        if return_map:\n",
    "            return dist, attn_map\n",
    "        else:\n",
    "            return dist\n",
    "\n",
    "    def local_init(self, locality_strength=1.):\n",
    "        self.v.weight.data.copy_(torch.eye(self.dim))\n",
    "        locality_distance = 1 #max(1,1/locality_strength**.5)\n",
    "\n",
    "        kernel_size = int(self.num_heads**.5)\n",
    "        center = (kernel_size-1)/2 if kernel_size%2==0 else kernel_size//2\n",
    "        for h1 in range(kernel_size):\n",
    "            for h2 in range(kernel_size):\n",
    "                position = h1+kernel_size*h2\n",
    "                self.pos_proj.weight.data[position,2] = -1\n",
    "                self.pos_proj.weight.data[position,1] = 2*(h1-center)*locality_distance\n",
    "                self.pos_proj.weight.data[position,0] = 2*(h2-center)*locality_distance\n",
    "        self.pos_proj.weight.data *= locality_strength\n",
    "\n",
    "    def get_rel_indices(self, num_patches):\n",
    "        img_size = int(num_patches**.5)\n",
    "        rel_indices   = torch.zeros(1, num_patches, num_patches, 3)\n",
    "        ind = torch.arange(img_size).view(1,-1) - torch.arange(img_size).view(-1, 1)\n",
    "        indx = ind.repeat(img_size,img_size)\n",
    "        indy = ind.repeat_interleave(img_size,dim=0).repeat_interleave(img_size,dim=1)\n",
    "        indd = indx**2 + indy**2\n",
    "        rel_indices[:,:,:,2] = indd.unsqueeze(0)\n",
    "        rel_indices[:,:,:,1] = indy.unsqueeze(0)\n",
    "        rel_indices[:,:,:,0] = indx.unsqueeze(0)\n",
    "        device = self.qk.weight.device\n",
    "        self.rel_indices = rel_indices.to(device)\n",
    "\n",
    "\n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., fc=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def get_attention_map(self, x, return_map = False):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn_map = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn_map = attn_map.softmax(dim=-1).mean(0)\n",
    "\n",
    "        img_size = int(N**.5)\n",
    "        ind = torch.arange(img_size).view(1,-1) - torch.arange(img_size).view(-1, 1)\n",
    "        indx = ind.repeat(img_size,img_size)\n",
    "        indy = ind.repeat_interleave(img_size,dim=0).repeat_interleave(img_size,dim=1)\n",
    "        indd = indx**2 + indy**2\n",
    "        distances = indd**.5\n",
    "        distances = distances.to('cuda')\n",
    "\n",
    "        dist = torch.einsum('nm,hnm->h', (distances, attn_map))\n",
    "        dist /= N\n",
    "\n",
    "        if return_map:\n",
    "            return dist, attn_map\n",
    "        else:\n",
    "            return dist\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class ScaleNorm(nn.Module):\n",
    "    \"\"\"See\n",
    "    https://github.com/lucidrains/reformer-pytorch/blob/a751fe2eb939dcdd81b736b2f67e745dc8472a09/reformer_pytorch/reformer_pytorch.py#L143\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(1))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        n = torch.norm(x, dim=-1, keepdim=True).clamp(min=self.eps)\n",
    "        return x / n * self.g\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads,  mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, attention_type=GPSA,\n",
    "                 fc=nn.Linear, **kwargs):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = attention_type(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, fc=fc, **kwargs)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop, fc=fc)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.norm1.reset_parameters()\n",
    "        self.norm2.reset_parameters()\n",
    "        self.attn.reset_parameters()\n",
    "        self.mlp.apply(self.mlp._init_weights)\n",
    "\n",
    "    def forward(self, x, mask_heads=None, task_index=1, attn_mask=None):\n",
    "     if isinstance(self.attn, (ClassAttention, JointCA)):\n",
    "        cls_token = x[:, :task_index]\n",
    "        xx = self.norm1(x)\n",
    "        xx, attn, v = self.attn(xx, mask_heads=mask_heads, nb=task_index, attn_mask=attn_mask)\n",
    "        cls_token = self.drop_path(xx[:, :task_index]) + cls_token\n",
    "        cls_token = self.drop_path(self.mlp(self.norm2(cls_token))) + cls_token\n",
    "        return cls_token, attn, v\n",
    "\n",
    "     xx = self.norm1(x)\n",
    "     out = self.attn(xx)\n",
    "\n",
    "    # Safe unpack\n",
    "     if isinstance(out, tuple):\n",
    "        if len(out) >= 3:\n",
    "            xx, attn, v = out[:3]\n",
    "        elif len(out) == 2:\n",
    "            xx, attn = out\n",
    "            v = None\n",
    "        else:\n",
    "            xx = out[0]\n",
    "            attn = v = None\n",
    "     else:\n",
    "        xx = out\n",
    "        attn = v = None\n",
    "\n",
    "     x = self.drop_path(xx) + x\n",
    "     x = self.drop_path(self.mlp(self.norm2(x))) + x\n",
    "     return x, attn, v\n",
    "\n",
    "\n",
    "\n",
    "class ClassAttention(nn.Module):\n",
    "    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "    # with slight modifications to do CA\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., fc=nn.Linear):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.q = fc(dim, dim, bias=qkv_bias)\n",
    "        self.k = fc(dim, dim, bias=qkv_bias)\n",
    "        self.v = fc(dim, dim, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = fc(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x, mask_heads=None, **kwargs):\n",
    "        B, N, C = x.shape\n",
    "        q = self.q(x[:,0]).unsqueeze(1).reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        k = self.k(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        q = q * self.scale\n",
    "        v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        if mask_heads is not None:\n",
    "            mask_heads = mask_heads.expand(B, self.num_heads, -1, N)\n",
    "            attn = attn * mask_heads\n",
    "\n",
    "        x_cls = (attn @ v).transpose(1, 2).reshape(B, 1, C)\n",
    "        x_cls = self.proj(x_cls)\n",
    "        x_cls = self.proj_drop(x_cls)\n",
    "\n",
    "        return x_cls, attn, v\n",
    "\n",
    "\n",
    "class JointCA(nn.Module):\n",
    "    \"\"\"Forward all task tokens together.\n",
    "\n",
    "    It uses a masked attention so that task tokens don't interact between them.\n",
    "    It should have the same results as independent forward per task token but being\n",
    "    much faster.\n",
    "\n",
    "    HOWEVER, it works a bit worse (like ~2pts less in 'all top-1' CIFAR100 50 steps).\n",
    "    So if anyone knows why, please tell me!\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., fc=nn.Linear):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.q = fc(dim, dim, bias=qkv_bias)\n",
    "        self.k = fc(dim, dim, bias=qkv_bias)\n",
    "        self.v = fc(dim, dim, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = fc(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @lru_cache(maxsize=1)\n",
    "    def get_attention_mask(self, attn_shape, nb_task_tokens):\n",
    "        \"\"\"Mask so that task tokens don't interact together.\n",
    "\n",
    "        Given two task tokens (t1, t2) and three patch tokens (p1, p2, p3), the\n",
    "        attention matrix is:\n",
    "\n",
    "        t1-t1 t1-t2 t1-p1 t1-p2 t1-p3\n",
    "        t2-t1 t2-t2 t2-p1 t2-p2 t2-p3\n",
    "\n",
    "        So that the mask (True values are deleted) should be:\n",
    "\n",
    "        False True False False False\n",
    "        True False False False False\n",
    "        \"\"\"\n",
    "        mask = torch.zeros(attn_shape, dtype=torch.bool)\n",
    "        for i in range(nb_task_tokens):\n",
    "            mask[:, i, :i] = True\n",
    "            mask[:, i, i+1:nb_task_tokens] = True\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x, attn_mask=False, nb_task_tokens=1, **kwargs):\n",
    "        B, N, C = x.shape\n",
    "        q = self.q(x[:,:nb_task_tokens]).reshape(B, nb_task_tokens, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        k = self.k(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        q = q * self.scale\n",
    "        v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "        if attn_mask:\n",
    "            mask = self.get_attention_mask(attn.shape, nb_task_tokens)\n",
    "            attn[mask] = -float('inf')\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x_cls = (attn @ v).transpose(1, 2).reshape(B, nb_task_tokens, C)\n",
    "        x_cls = self.proj(x_cls)\n",
    "        x_cls = self.proj_drop(x_cls)\n",
    "\n",
    "        return x_cls, attn, v\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding, from timm\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        #assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "        #    f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class HybridEmbed(nn.Module):\n",
    "    \"\"\" CNN Feature Map Embedding, from timm\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone, img_size=224, feature_size=None, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        assert isinstance(backbone, nn.Module)\n",
    "        img_size = to_2tuple(img_size)\n",
    "        self.img_size = img_size\n",
    "        self.backbone = backbone\n",
    "        if feature_size is None:\n",
    "            with torch.no_grad():\n",
    "                training = backbone.training\n",
    "                if training:\n",
    "                    backbone.eval()\n",
    "                o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))[-1]\n",
    "                feature_size = o.shape[-2:]\n",
    "                feature_dim = o.shape[1]\n",
    "                backbone.train(training)\n",
    "        else:\n",
    "            feature_size = to_2tuple(feature_size)\n",
    "            feature_dim = self.backbone.feature_info.channels()[-1]\n",
    "        self.num_patches = feature_size[0] * feature_size[1]\n",
    "        self.proj = nn.Linear(feature_dim, embed_dim)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)[-1]\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConVit(nn.Module):\n",
    "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., hybrid_backbone=None, norm_layer='layer',\n",
    "                 local_up_to_layer=3, locality_strength=1., use_pos_embed=True,\n",
    "                 class_attention=False, ca_type='base',\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.local_up_to_layer = local_up_to_layer\n",
    "        self.num_features = self.final_dim = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.locality_strength = locality_strength\n",
    "        self.use_pos_embed = use_pos_embed\n",
    "\n",
    "        if norm_layer == 'layer':\n",
    "            norm_layer = nn.LayerNorm\n",
    "        elif norm_layer == 'scale':\n",
    "            norm_layer = ScaleNorm\n",
    "        else:\n",
    "            raise NotImplementedError(f'Unknown normalization {norm_layer}')\n",
    "\n",
    "        if hybrid_backbone is not None:\n",
    "            self.patch_embed = HybridEmbed(\n",
    "                hybrid_backbone, img_size=img_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        else:\n",
    "            self.patch_embed = PatchEmbed(\n",
    "                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        if self.use_pos_embed:\n",
    "            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.pos_embed, std=.02)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "\n",
    "        blocks = []\n",
    "\n",
    "        if ca_type == 'base':\n",
    "            ca_block = ClassAttention\n",
    "        elif ca_type == 'jointca':\n",
    "            ca_block = JointCA\n",
    "        else:\n",
    "            raise ValueError(f'Unknown CA type {ca_type}')\n",
    "\n",
    "        for layer_index in range(depth):\n",
    "            if layer_index < local_up_to_layer:\n",
    "                # Convit\n",
    "                block = Block(\n",
    "                    dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                    drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[layer_index], norm_layer=norm_layer,\n",
    "                    attention_type=GPSA, locality_strength=locality_strength\n",
    "                )\n",
    "            elif not class_attention:\n",
    "                # Convit\n",
    "                block = Block(\n",
    "                    dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                    drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[layer_index], norm_layer=norm_layer,\n",
    "                    attention_type=MHSA\n",
    "                )\n",
    "            else:\n",
    "                # CaiT\n",
    "                block = Block(\n",
    "                    dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                    drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[layer_index], norm_layer=norm_layer,\n",
    "                    attention_type=ca_block\n",
    "                )\n",
    "\n",
    "            blocks.append(block)\n",
    "\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        self.use_class_attention = class_attention\n",
    "\n",
    "        # Classifier head\n",
    "        self.feature_info = [dict(num_chs=embed_dim, reduction=0, module='head')]\n",
    "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.head.apply(self._init_weights)\n",
    "\n",
    "    def freeze(self, names):\n",
    "        for name in names:\n",
    "            if name == 'all':\n",
    "                return cutils.freeze_parameters(self)\n",
    "            elif name == 'old_heads':\n",
    "                self.head.freeze(name)\n",
    "            elif name == 'backbone':\n",
    "                cutils.freeze_parameters(self.blocks)\n",
    "                cutils.freeze_parameters(self.patch_embed)\n",
    "                cutils.freeze_parameters(self.pos_embed)\n",
    "                cutils.freeze_parameters(self.norm)\n",
    "            else:\n",
    "                raise NotImplementedError(f'Unknown name={name}.')\n",
    "\n",
    "    def reset_classifier(self):\n",
    "        self.head.apply(self._init_weights)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for b in self.blocks:\n",
    "            b.reset_parameters()\n",
    "        self.norm.reset_parameters()\n",
    "        self.head.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def get_internal_losses(self, clf_loss):\n",
    "        return {}\n",
    "\n",
    "    def end_finetuning(self):\n",
    "        pass\n",
    "\n",
    "    def begin_finetuning(self):\n",
    "        pass\n",
    "\n",
    "    def epoch_log(self):\n",
    "        return {}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def forward_sa(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        if self.use_pos_embed:\n",
    "            x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks[:self.local_up_to_layer]:\n",
    "            x, _ = blk(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_features(self, x, final_norm=True):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "\n",
    "        if self.use_pos_embed:\n",
    "            x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks[:self.local_up_to_layer]:\n",
    "            x, _, _ = blk(x)\n",
    "\n",
    "        if self.use_class_attention:\n",
    "            for blk in self.blocks[self.local_up_to_layer:]:\n",
    "                cls_tokens, _, _ = blk(torch.cat((cls_tokens, x), dim=1))\n",
    "        else:\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "            for blk in self.blocks[self.local_up_to_layer:]:\n",
    "                x, _ , _ = blk(x)\n",
    "\n",
    "        if final_norm:\n",
    "            if self.use_class_attention:\n",
    "                cls_tokens = self.norm(cls_tokens)\n",
    "            else:\n",
    "                x = self.norm(x)\n",
    "\n",
    "        if self.use_class_attention:\n",
    "            return cls_tokens[:, 0], None, None\n",
    "        else:\n",
    "            return x[:, 0], None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)[0]\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:25:05.571794Z",
     "iopub.status.busy": "2025-11-01T16:25:05.571513Z",
     "iopub.status.idle": "2025-11-01T16:25:08.562951Z",
     "shell.execute_reply": "2025-11-01T16:25:08.562123Z",
     "shell.execute_reply.started": "2025-11-01T16:25:05.571772Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DyTox ConViT backbone loaded from ImageNet pretrained model.\n",
      "Missing keys: ['blocks.0.attn.qk.bias', 'blocks.0.attn.v.bias', 'blocks.1.attn.qk.bias', 'blocks.1.attn.v.bias', 'blocks.2.attn.qk.bias', 'blocks.2.attn.v.bias', 'blocks.3.attn.q.weight', 'blocks.3.attn.q.bias', 'blocks.3.attn.k.weight', 'blocks.3.attn.k.bias', 'blocks.3.attn.v.bias', 'blocks.4.attn.q.weight', 'blocks.4.attn.q.bias', 'blocks.4.attn.k.weight', 'blocks.4.attn.k.bias', 'blocks.4.attn.v.bias', 'blocks.5.attn.q.weight', 'blocks.5.attn.q.bias', 'blocks.5.attn.k.weight', 'blocks.5.attn.k.bias', 'blocks.5.attn.v.bias', 'blocks.6.attn.q.weight', 'blocks.6.attn.q.bias', 'blocks.6.attn.k.weight', 'blocks.6.attn.k.bias', 'blocks.6.attn.v.bias', 'blocks.7.attn.q.weight', 'blocks.7.attn.q.bias', 'blocks.7.attn.k.weight', 'blocks.7.attn.k.bias', 'blocks.7.attn.v.bias', 'blocks.8.attn.q.weight', 'blocks.8.attn.q.bias', 'blocks.8.attn.k.weight', 'blocks.8.attn.k.bias', 'blocks.8.attn.v.bias', 'blocks.9.attn.q.weight', 'blocks.9.attn.q.bias', 'blocks.9.attn.k.weight', 'blocks.9.attn.k.bias', 'blocks.9.attn.v.bias', 'blocks.10.attn.q.weight', 'blocks.10.attn.q.bias', 'blocks.10.attn.k.weight', 'blocks.10.attn.k.bias', 'blocks.10.attn.v.weight', 'blocks.10.attn.v.bias', 'blocks.11.attn.q.weight', 'blocks.11.attn.q.bias', 'blocks.11.attn.k.weight', 'blocks.11.attn.k.bias', 'blocks.11.attn.v.weight', 'blocks.11.attn.v.bias', 'head.weight', 'head.bias']\n",
      "Unexpected keys: ['blocks.3.attn.gating_param', 'blocks.3.attn.qk.weight', 'blocks.3.attn.pos_proj.weight', 'blocks.3.attn.pos_proj.bias', 'blocks.4.attn.gating_param', 'blocks.4.attn.qk.weight', 'blocks.4.attn.pos_proj.weight', 'blocks.4.attn.pos_proj.bias', 'blocks.5.attn.gating_param', 'blocks.5.attn.qk.weight', 'blocks.5.attn.pos_proj.weight', 'blocks.5.attn.pos_proj.bias', 'blocks.6.attn.gating_param', 'blocks.6.attn.qk.weight', 'blocks.6.attn.pos_proj.weight', 'blocks.6.attn.pos_proj.bias', 'blocks.7.attn.gating_param', 'blocks.7.attn.qk.weight', 'blocks.7.attn.pos_proj.weight', 'blocks.7.attn.pos_proj.bias', 'blocks.8.attn.gating_param', 'blocks.8.attn.qk.weight', 'blocks.8.attn.pos_proj.weight', 'blocks.8.attn.pos_proj.bias', 'blocks.9.attn.gating_param', 'blocks.9.attn.qk.weight', 'blocks.9.attn.pos_proj.weight', 'blocks.9.attn.pos_proj.bias', 'blocks.10.attn.qkv.weight', 'blocks.11.attn.qkv.weight']\n"
     ]
    }
   ],
   "source": [
    "from timm import create_model\n",
    "\n",
    "# 1️⃣ Load pretrained convit_base from timm\n",
    "timm_convit = create_model(\"convit_base\", pretrained=True)\n",
    "\n",
    "# 2️⃣ Create DyTox-compatible ConViT\n",
    "dytox_convit = ConVit(\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    in_chans=3,\n",
    "    num_classes=2,  # binary classification\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    num_heads=16,   # matches convit_base\n",
    "    mlp_ratio=4.0,\n",
    "    qkv_bias=True,\n",
    "    local_up_to_layer=3,\n",
    "    locality_strength=1.0,\n",
    "    use_pos_embed=True,\n",
    "    class_attention=True,\n",
    ")\n",
    "\n",
    "# 3️⃣ Filter out classifier weights (head.*)\n",
    "state_dict = timm_convit.state_dict()\n",
    "filtered_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"head.\")}\n",
    "\n",
    "# 4️⃣ Load pretrained backbone\n",
    "missing, unexpected = dytox_convit.load_state_dict(filtered_dict, strict=False)\n",
    "\n",
    "print(\"✅ DyTox ConViT backbone loaded from ImageNet pretrained model.\")\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:25:13.510341Z",
     "iopub.status.busy": "2025-11-01T16:25:13.510061Z",
     "iopub.status.idle": "2025-11-01T16:25:13.546444Z",
     "shell.execute_reply": "2025-11-01T16:25:13.545662Z",
     "shell.execute_reply.started": "2025-11-01T16:25:13.510321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from timm.models.layers import trunc_normal_\n",
    "from torch import nn\n",
    "\n",
    "import continual.utils as cutils\n",
    "\n",
    "class ContinualClassifier(nn.Module):\n",
    "   \n",
    "    def __init__(self, embed_dim, nb_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.nb_classes = nb_classes\n",
    "        self.head = nn.Linear(embed_dim, nb_classes, bias=True)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.head.reset_parameters()\n",
    "        self.norm.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.head(x)\n",
    "\n",
    "    def add_new_outputs(self, n):\n",
    "        head = nn.Linear(self.embed_dim, self.nb_classes + n, bias=True)\n",
    "        head.weight.data[:-n] = self.head.weight.data\n",
    "\n",
    "        head.to(self.head.weight.device)\n",
    "        self.head = head\n",
    "        self.nb_classes += n\n",
    "\n",
    "def _get_module_device(mod):\n",
    "     for p in mod.parameters():\n",
    "        return p.device\n",
    "    # fallback\n",
    "     return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class DyTox(nn.Module):\n",
    "   \n",
    "    def __init__(\n",
    "        self,\n",
    "        transformer,\n",
    "        nb_classes,\n",
    "        individual_classifier='',\n",
    "        head_div=False,\n",
    "        head_div_mode=['tr', 'ft'],\n",
    "        joint_tokens=False,\n",
    "        resnet=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nb_classes = nb_classes\n",
    "        self.embed_dim = transformer.embed_dim\n",
    "        self.individual_classifier = individual_classifier\n",
    "        self.use_head_div = head_div\n",
    "        self.head_div_mode = head_div_mode\n",
    "        self.head_div = None\n",
    "        self.joint_tokens = joint_tokens\n",
    "        self.in_finetuning = False\n",
    "\n",
    "        self.use_resnet = resnet\n",
    "\n",
    "        self.nb_classes_per_task = [nb_classes]\n",
    "\n",
    "        if self.use_resnet:\n",
    "            print('ResNet18 backbone for ens')\n",
    "            # self.backbone = resnet18()\n",
    "            # self.backbone.head = nn.Sequential(\n",
    "            #     nn.Conv2d(256, 384, kernel_size=1),\n",
    "            #     nn.BatchNorm2d(384),\n",
    "            #     nn.ReLU(inplace=True),\n",
    "            #     nn.Conv2d(384, 504, kernel_size=1),\n",
    "            #     nn.BatchNorm2d(504),\n",
    "            #     nn.ReLU(inplace=True)\n",
    "            # )\n",
    "            # self.backbone.avgpool = nn.Identity()\n",
    "            # self.backbone.layer4 = nn.Identity()\n",
    "            # #self.backbone.layer4 = self.backbone._make_layer_nodown(\n",
    "            # #    256, 512, 2, stride=1, dilation=2\n",
    "            # #)\n",
    "            # self.backbone = self.backbone.cuda()\n",
    "            # self.backbone.embed_dim = 504\n",
    "            # self.embed_dim = self.backbone.embed_dim\n",
    "\n",
    "            # self.tabs = nn.ModuleList([\n",
    "            #     Block(\n",
    "            #         dim=self.embed_dim, num_heads=12, mlp_ratio=4, qkv_bias=False, qk_scale=None,\n",
    "            #         drop=0., attn_drop=0., drop_path=0., norm_layer=nn.LayerNorm,\n",
    "            #         attention_type=ClassAttention\n",
    "            #     ).cuda()\n",
    "            # ])\n",
    "            # self.tabs[0].reset_parameters()\n",
    "\n",
    "            # token = nn.Parameter(torch.zeros(1, 1, self.embed_dim).cuda())\n",
    "            # trunc_normal_(token, std=.02)\n",
    "            # self.task_tokens = nn.ParameterList([token])\n",
    "        else:\n",
    "            self.patch_embed = transformer.patch_embed\n",
    "            self.pos_embed = transformer.pos_embed\n",
    "            self.pos_drop = transformer.pos_drop\n",
    "            self.sabs = transformer.blocks[:transformer.local_up_to_layer]\n",
    "            self.tabs = transformer.blocks[transformer.local_up_to_layer:]\n",
    "            self.task_tokens = nn.ParameterList([transformer.cls_token])\n",
    "\n",
    "        if self.individual_classifier != '':\n",
    "            in_dim, out_dim = self._get_ind_clf_dim()\n",
    "            self.head = nn.ModuleList([\n",
    "                ContinualClassifier(in_dim, out_dim).cuda()\n",
    "            ])\n",
    "        else:\n",
    "            self.head = ContinualClassifier(\n",
    "                self.embed_dim * len(self.task_tokens), sum(self.nb_classes_per_task)\n",
    "            ).cuda()\n",
    "\n",
    "    def end_finetuning(self):\n",
    "        \"\"\"Start FT mode, usually with backbone freezed and balanced classes.\"\"\"\n",
    "        self.in_finetuning = False\n",
    "\n",
    "    def begin_finetuning(self):\n",
    "        \"\"\"End FT mode, usually with backbone freezed and balanced classes.\"\"\"\n",
    "        self.in_finetuning = True\n",
    "\n",
    "    # def add_model(self, nb_new_classes):\n",
    "    #     \"\"\"Expand model as per the DyTox framework given `nb_new_classes`.\n",
    "\n",
    "    #     :param nb_new_classes: Number of new classes brought by the new task.\n",
    "    #     \"\"\"\n",
    "    #     self.nb_classes_per_task.append(nb_new_classes)\n",
    "\n",
    "    #     # Class tokens ---------------------------------------------------------\n",
    "    #     new_task_token = copy.deepcopy(self.task_tokens[-1])\n",
    "    #     trunc_normal_(new_task_token, std=.02)\n",
    "    #     self.task_tokens.append(new_task_token)\n",
    "    #     # ----------------------------------------------------------------------\n",
    "\n",
    "    #     # Diversity head -------------------------------------------------------\n",
    "    #     if self.use_head_div:\n",
    "    #         self.head_div = ContinualClassifier(\n",
    "    #             self.embed_dim, self.nb_classes_per_task[-1] + 1\n",
    "    #         ).cuda()\n",
    "    #     # ----------------------------------------------------------------------\n",
    "\n",
    "    #     # Classifier -----------------------------------------------------------\n",
    "    #     if self.individual_classifier != '':\n",
    "    #         in_dim, out_dim = self._get_ind_clf_dim()\n",
    "    #         self.head.append(\n",
    "    #             ContinualClassifier(in_dim, out_dim).cuda()\n",
    "    #         )\n",
    "    #     else:\n",
    "    #         self.head = ContinualClassifier(\n",
    "    #             self.embed_dim * len(self.task_tokens), sum(self.nb_classes_per_task)\n",
    "    #         ).cuda()\n",
    "    #     # ----------------------------------------------------------------------\n",
    "\n",
    "    def _get_ind_clf_dim(self):\n",
    "        \"\"\"What are the input and output dim of classifier depending on its config.\n",
    "\n",
    "        By default, DyTox is in 1-1.\n",
    "        \"\"\"\n",
    "        if self.individual_classifier == '1-1':\n",
    "            in_dim = self.embed_dim\n",
    "            out_dim = self.nb_classes_per_task[-1]\n",
    "        elif self.individual_classifier == '1-n':\n",
    "            in_dim = self.embed_dim\n",
    "            out_dim = sum(self.nb_classes_per_task)\n",
    "        elif self.individual_classifier == 'n-n':\n",
    "            in_dim = len(self.task_tokens) * self.embed_dim\n",
    "            out_dim = sum(self.nb_classes_per_task)\n",
    "        elif self.individual_classifier == 'n-1':\n",
    "            in_dim = len(self.task_tokens) * self.embed_dim\n",
    "            out_dim = self.nb_classes_per_task[-1]\n",
    "        else:\n",
    "            raise NotImplementedError(f'Unknown ind classifier {self.individual_classifier}')\n",
    "        return in_dim, out_dim\n",
    "\n",
    "    def freeze(self, names):\n",
    "        \"\"\"Choose what to freeze depending on the name of the module.\"\"\"\n",
    "        requires_grad = False\n",
    "        cutils.freeze_parameters(self, requires_grad=not requires_grad)\n",
    "        self.train()\n",
    "\n",
    "        for name in names:\n",
    "            if name == 'all':\n",
    "                self.eval()\n",
    "                return cutils.freeze_parameters(self)\n",
    "            elif name == 'old_task_tokens':\n",
    "                cutils.freeze_parameters(self.task_tokens[:-1], requires_grad=requires_grad)\n",
    "            elif name == 'task_tokens':\n",
    "                cutils.freeze_parameters(self.task_tokens, requires_grad=requires_grad)\n",
    "            elif name == 'sab':\n",
    "                if self.use_resnet:\n",
    "                    self.backbone.eval()\n",
    "                    cutils.freeze_parameters(self.backbone, requires_grad=requires_grad)\n",
    "                else:\n",
    "                    self.sabs.eval()\n",
    "                    cutils.freeze_parameters(self.patch_embed, requires_grad=requires_grad)\n",
    "                    cutils.freeze_parameters(self.pos_embed, requires_grad=requires_grad)\n",
    "                    cutils.freeze_parameters(self.sabs, requires_grad=requires_grad)\n",
    "            elif name == 'tab':\n",
    "                self.tabs.eval()\n",
    "                cutils.freeze_parameters(self.tabs, requires_grad=requires_grad)\n",
    "            elif name == 'old_heads':\n",
    "                self.head[:-1].eval()\n",
    "                cutils.freeze_parameters(self.head[:-1], requires_grad=requires_grad)\n",
    "            elif name == 'heads':\n",
    "                self.head.eval()\n",
    "                cutils.freeze_parameters(self.head, requires_grad=requires_grad)\n",
    "            elif name == 'head_div':\n",
    "                self.head_div.eval()\n",
    "                cutils.freeze_parameters(self.head_div, requires_grad=requires_grad)\n",
    "            else:\n",
    "                raise NotImplementedError(f'Unknown name={name}.')\n",
    "\n",
    "    def param_groups(self):\n",
    "        return {\n",
    "            'all': self.parameters(),\n",
    "            'old_task_tokens': self.task_tokens[:-1],\n",
    "            'task_tokens': self.task_tokens.parameters(),\n",
    "            'new_task_tokens': [self.task_tokens[-1]],\n",
    "            'sa': self.sabs.parameters(),\n",
    "            'patch': self.patch_embed.parameters(),\n",
    "            'pos': [self.pos_embed],\n",
    "            'ca': self.tabs.parameters(),\n",
    "            'old_heads': self.head[:-self.nb_classes_per_task[-1]].parameters() \\\n",
    "                              if self.individual_classifier else \\\n",
    "                              self.head.parameters(),\n",
    "            'new_head': self.head[-1].parameters() if self.individual_classifier else self.head.parameters(),\n",
    "            'head': self.head.parameters(),\n",
    "            'head_div': self.head_div.parameters() if self.head_div is not None else None\n",
    "        }\n",
    "\n",
    "    def reset_classifier(self):\n",
    "        if isinstance(self.head, nn.ModuleList):\n",
    "            for head in self.head:\n",
    "                head.reset_parameters()\n",
    "        else:\n",
    "            self.head.reset_parameters()\n",
    "\n",
    "    def hook_before_update(self):\n",
    "        pass\n",
    "\n",
    "    def hook_after_update(self):\n",
    "        pass\n",
    "\n",
    "    def hook_after_epoch(self):\n",
    "        pass\n",
    "\n",
    "    def epoch_log(self):\n",
    "        \"\"\"Write here whatever you want to log on the internal state of the model.\"\"\"\n",
    "        log = {}\n",
    "\n",
    "        # Compute mean distance between class tokens\n",
    "        mean_dist, min_dist, max_dist = [], float('inf'), 0.\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(self.task_tokens)):\n",
    "                for j in range(i + 1, len(self.task_tokens)):\n",
    "                    dist = torch.norm(self.task_tokens[i] - self.task_tokens[j], p=2).item()\n",
    "                    mean_dist.append(dist)\n",
    "\n",
    "                    min_dist = min(dist, min_dist)\n",
    "                    max_dist = max(dist, max_dist)\n",
    "\n",
    "        if len(mean_dist) > 0:\n",
    "            mean_dist = sum(mean_dist) / len(mean_dist)\n",
    "        else:\n",
    "            mean_dist = 0.\n",
    "            min_dist = 0.\n",
    "\n",
    "        assert min_dist <= mean_dist <= max_dist, (min_dist, mean_dist, max_dist)\n",
    "        log['token_mean_dist'] = round(mean_dist, 5)\n",
    "        log['token_min_dist'] = round(min_dist, 5)\n",
    "        log['token_max_dist'] = round(max_dist, 5)\n",
    "        return log\n",
    "\n",
    "    def get_internal_losses(self, clf_loss):\n",
    "        \"\"\"If you want to compute some internal loss, like a EWC loss for example.\n",
    "\n",
    "        :param clf_loss: The main classification loss (if you wanted to use its gradient for example).\n",
    "        :return: a dictionnary of losses, all values will be summed in the final loss.\n",
    "        \"\"\"\n",
    "        int_losses = {}\n",
    "        return int_losses\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        # Shared part, this is the ENCODER\n",
    "        B = x.shape[0]\n",
    "\n",
    "        if self.use_resnet:\n",
    "            x, self.feats = self.backbone.forward_tokens(x)\n",
    "        else:\n",
    "            x = self.patch_embed(x)\n",
    "            x = x + self.pos_embed\n",
    "            x = self.pos_drop(x)\n",
    "\n",
    "            self.feats = []\n",
    "            for blk in self.sabs:\n",
    "                x, attn, v = blk(x)\n",
    "                self.feats.append(x)\n",
    "            self.feats.pop(-1)\n",
    "\n",
    "        # Specific part, this is what we called the \"task specific DECODER\"\n",
    "        if self.joint_tokens:\n",
    "            return self.forward_features_jointtokens(x)\n",
    "\n",
    "        tokens = []\n",
    "        attentions = []\n",
    "        mask_heads = None\n",
    "\n",
    "        for task_token in self.task_tokens:\n",
    "            task_token = task_token.expand(B, -1, -1)\n",
    "\n",
    "            for blk in self.tabs:\n",
    "                task_token, attn, v = blk(torch.cat((task_token, x), dim=1), mask_heads=mask_heads)\n",
    "\n",
    "            attentions.append(attn)\n",
    "            tokens.append(task_token[:, 0])\n",
    "\n",
    "        self._class_tokens = tokens\n",
    "        return tokens, tokens[-1], attentions\n",
    "\n",
    "    def forward_features_jointtokens(self, x):\n",
    "        \"\"\"Method to do a single TAB forward with all task tokens.\n",
    "\n",
    "        A masking is used to avoid interaction between tasks. In theory it should\n",
    "        give the same results as multiple TAB forward, but in practice it's a little\n",
    "        bit worse, not sure why. So if you have an idea, please tell me!\n",
    "        \"\"\"\n",
    "        B = len(x)\n",
    "\n",
    "        task_tokens = torch.cat(\n",
    "            [task_token.expand(B, 1, -1) for task_token in self.task_tokens],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        for blk in self.tabs:\n",
    "            task_tokens, _, _ = blk(\n",
    "                torch.cat((task_tokens, x), dim=1),\n",
    "                task_index=len(self.task_tokens),\n",
    "                attn_mask=True\n",
    "            )\n",
    "\n",
    "        if self.individual_classifier in ('1-1', '1-n'):\n",
    "            return task_tokens.permute(1, 0, 2), task_tokens[:, -1], None\n",
    "        return task_tokens.view(B, -1), task_tokens[:, -1], None\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Helper: get current device\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# New add_model implementation\n",
    "# -------------------------\n",
    "    def add_model(self, nb_new_classes):\n",
    "     \"\"\"Expand model safely (device-aware).\"\"\"\n",
    "    # record new classes\n",
    "     self.nb_classes_per_task.append(int(nb_new_classes))\n",
    "\n",
    "    # device to place newly created params\n",
    "     device = _get_module_device(self)\n",
    "\n",
    "    # --- new class token ---\n",
    "     new_task_token = copy.deepcopy(self.task_tokens[-1])\n",
    "     trunc_normal_(new_task_token, std=.02)\n",
    "    # place on device\n",
    "     new_task_token = new_task_token.to(device)\n",
    "     self.task_tokens.append(new_task_token)\n",
    "    # ------------------------\n",
    "\n",
    "    # --- diversity head (optional) ---\n",
    "     if self.use_head_div:\n",
    "        self.head_div = ContinualClassifier(self.embed_dim, self.nb_classes_per_task[-1] + 1)\n",
    "        self.head_div.to(device)\n",
    "        self.head_div.reset_parameters()\n",
    "    # ---------------------------------\n",
    "\n",
    "    # --- classifier expansion ---\n",
    "     if self.individual_classifier != '':\n",
    "        # individual heads per task (1-1 typical)\n",
    "        in_dim, out_dim = self._get_ind_clf_dim()\n",
    "        new_head = ContinualClassifier(in_dim, out_dim)\n",
    "        new_head.to(device)\n",
    "        new_head.reset_parameters()\n",
    "        # append to ModuleList\n",
    "        if not isinstance(self.head, nn.ModuleList):\n",
    "            # convert single head -> ModuleList (edge-case)\n",
    "            old = self.head\n",
    "            self.head = nn.ModuleList([old])\n",
    "        self.head.append(new_head)\n",
    "     else:\n",
    "        # single-head case: expand output dim\n",
    "        # create a new head with expanded outputs and copy weights\n",
    "        total_out = sum(self.nb_classes_per_task)\n",
    "        new_head = ContinualClassifier(self.embed_dim * len(self.task_tokens), total_out)\n",
    "        new_head.to(device)\n",
    "        # If previous head exists, try to copy weights for the shared dims\n",
    "        try:\n",
    "            # operate on cpu temporarily to avoid device issues\n",
    "            old_w = self.head.head.weight.data.clone()\n",
    "            old_b = self.head.head.bias.data.clone()\n",
    "            new_head.head.weight.data[:old_w.shape[0]] = old_w\n",
    "            new_head.head.bias.data[:old_b.shape[0]] = old_b\n",
    "        except Exception:\n",
    "            # ignore if incompatible, reset instead\n",
    "            new_head.reset_parameters()\n",
    "        self.head = new_head\n",
    "    # -----------------------------\n",
    "\n",
    "# -------------------------\n",
    "# Safe forward_classifier\n",
    "# -------------------------\n",
    "    def forward_classifier(self, tokens, last_token):\n",
    "     \"\"\"\n",
    "     tokens: list of per-task embeddings (each shape [B, embed_dim])\n",
    "     last_token: tokens[-1]\n",
    "     Returns dict{'logits','div','tokens'} (logits on correct device)\n",
    "     \"\"\"\n",
    "     device = tokens[0].device if isinstance(tokens, (list, tuple)) and len(tokens) > 0 else _get_module_device(self)\n",
    "     logits_div = None\n",
    "\n",
    "     if self.individual_classifier != '':\n",
    "        # Expect self.head to be ModuleList with one head per task\n",
    "        logits_list = []\n",
    "        for i, head in enumerate(self.head):\n",
    "            if self.individual_classifier in ('1-n', '1-1'):\n",
    "                logits_list.append(head(tokens[i]))  # each -> [B, num_classes_i]\n",
    "            else:\n",
    "                # n-1 or n-n: concatenate tokens up to i\n",
    "                concat = torch.cat(tokens[:i+1], dim=1)\n",
    "                logits_list.append(head(concat))\n",
    "\n",
    "        if self.individual_classifier in ('1-1', 'n-1'):\n",
    "            logits = torch.cat(logits_list, dim=1)\n",
    "        else:\n",
    "            # combine into shared final logits (original repo does some averaging)\n",
    "            final_logits = torch.zeros_like(logits_list[-1], device=device)\n",
    "            for i, l in enumerate(logits_list):\n",
    "                final_logits[:, :l.shape[1]] += l\n",
    "            for i, c in enumerate(self.nb_classes_per_task):\n",
    "                final_logits[:, :c] /= (len(self.nb_classes_per_task) - i)\n",
    "            logits = final_logits\n",
    "     elif isinstance(tokens, torch.Tensor):\n",
    "        logits = self.head(tokens)\n",
    "     else:\n",
    "        # single-head with tokens list: concatenate\n",
    "        concat = torch.cat(tokens, dim=1)\n",
    "        logits = self.head(concat)\n",
    "\n",
    "    # head div if present\n",
    "     if self.head_div is not None and eval_training_finetuning(self.head_div_mode, self.in_finetuning):\n",
    "        logits_div = self.head_div(last_token)\n",
    "\n",
    "     return {'logits': logits, 'div': logits_div, 'tokens': tokens}\n",
    "\n",
    "# -------------------------\n",
    "# set_task: mark current task\n",
    "# -------------------------\n",
    "    def set_task(self, task_id: int):\n",
    "    # simple setter used at train/eval time\n",
    "     self.current_task_id = int(task_id)\n",
    "\n",
    "# -------------------------\n",
    "# Safe forward: prefer current_task if set (1-1 typical)\n",
    "# -------------------------\n",
    "    def forward(self, x):\n",
    "     tokens, last_token, _ = self.forward_features(x)\n",
    "     if hasattr(self, \"current_task_id\") and self.individual_classifier in ('1-1', '1-n'):\n",
    "        # use only the active head (prevents accidentally scoring other heads)\n",
    "        idx = int(self.current_task_id)\n",
    "        logits = self.head[idx](tokens[idx])\n",
    "        logits_div = None\n",
    "        if self.head_div is not None and eval_training_finetuning(self.head_div_mode, self.in_finetuning):\n",
    "            logits_div = self.head_div(last_token)\n",
    "        return {'logits': logits, 'div': logits_div, 'tokens': tokens}\n",
    "    # fallback to full classifier behavior\n",
    "     return self.forward_classifier(tokens, last_token)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def eval_training_finetuning(mode, in_ft):\n",
    "    if 'tr' in mode and 'ft' in mode:\n",
    "        return True\n",
    "    if 'tr' in mode and not in_ft:\n",
    "        return True\n",
    "    if 'ft' in mode and in_ft:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:25:21.771016Z",
     "iopub.status.busy": "2025-11-01T16:25:21.770413Z",
     "iopub.status.idle": "2025-11-01T16:25:21.990367Z",
     "shell.execute_reply": "2025-11-01T16:25:21.989537Z",
     "shell.execute_reply.started": "2025-11-01T16:25:21.770991Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = DyTox(\n",
    "    transformer=dytox_convit,\n",
    "    nb_classes=2,                   # 🔥 Important\n",
    "    individual_classifier='1-1',\n",
    "    head_div=False,\n",
    "    head_div_mode=['tr', 'ft'],\n",
    "    joint_tokens=False,\n",
    "    resnet=False\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:25:25.706017Z",
     "iopub.status.busy": "2025-11-01T16:25:25.705531Z",
     "iopub.status.idle": "2025-11-01T16:25:25.717983Z",
     "shell.execute_reply": "2025-11-01T16:25:25.717260Z",
     "shell.execute_reply.started": "2025-11-01T16:25:25.705994Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropyBoosting(nn.Module):\n",
    "    \"\"\"\n",
    "    NLL loss with label smoothing.\n",
    "    \"\"\"\n",
    "    def __init__(self, smoothing=0.1, alpha=1, gamma=1):\n",
    "        \"\"\"\n",
    "        Constructor for the LabelSmoothing module.\n",
    "        :param smoothing: label smoothing factor\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert smoothing < 1.0\n",
    "        self.smoothing = smoothing\n",
    "        self.confidence = 1. - smoothing\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, x, target, boosting_output=None, boosting_focal=None):\n",
    "        if boosting_output is None:\n",
    "            return self._base_loss(x, target)\n",
    "        return self._focal_loss(x, target, boosting_output, boosting_focal)\n",
    "\n",
    "    def _focal_loss(self, x, target, boosting_output, boosting_focal):\n",
    "        logprobs = F.log_softmax(x, dim=-1)\n",
    "\n",
    "        if boosting_focal == 'old':\n",
    "            pt = boosting_output.softmax(-1)[..., :-1]\n",
    "\n",
    "            f = torch.ones_like(logprobs)\n",
    "            f[:, :boosting_output.shape[1] - 1] = self.alpha * (1 - pt) ** self.gamma\n",
    "            logprobs = f * logprobs\n",
    "        elif boosting_focal == 'new':\n",
    "            pt = boosting_output.softmax(-1)[..., -1]\n",
    "            nb_old_classes = boosting_output.shape[1] - 1\n",
    "\n",
    "            f = torch.ones_like(logprobs)\n",
    "            f[:, nb_old_classes:] = self.alpha * (1 - pt[:, None]) ** self.gamma\n",
    "            logprobs = f * logprobs\n",
    "        else:\n",
    "            assert False, (boosting_focal)\n",
    "\n",
    "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -logprobs.mean(dim=-1)\n",
    "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()\n",
    "\n",
    "    def _base_loss(self, x, target):\n",
    "        logprobs = F.log_softmax(x, dim=-1)\n",
    "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -logprobs.mean(dim=-1)\n",
    "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class SoftTargetCrossEntropyBoosting(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha=1, gamma=1):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, x, target, boosting_output=None, boosting_focal=None):\n",
    "        if boosting_output is None:\n",
    "            return torch.sum(-target * F.log_softmax(x, dim=-1), dim=-1).mean()\n",
    "\n",
    "        if boosting_focal == 'old':\n",
    "            pt = boosting_output.softmax(-1)[..., :-1]\n",
    "\n",
    "            f = torch.ones_like(x)\n",
    "            f[:, :boosting_output.shape[1] - 1] = self.alpha * (1 - pt) ** self.gamma\n",
    "        elif boosting_focal == 'new':\n",
    "            pt = boosting_output.softmax(-1)[..., -1]\n",
    "\n",
    "            nb_old_classes = boosting_output.shape[1] - 1\n",
    "\n",
    "            f = torch.ones_like(x)\n",
    "            f[:, nb_old_classes:] = self.alpha * (1 - pt[:, None]) ** self.gamma\n",
    "        else:\n",
    "            assert False, (boosting_focal)\n",
    "\n",
    "        return torch.sum(-target * f * F.log_softmax(x, dim=-1), dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:25:31.719147Z",
     "iopub.status.busy": "2025-11-01T16:25:31.718456Z",
     "iopub.status.idle": "2025-11-01T16:25:31.732648Z",
     "shell.execute_reply": "2025-11-01T16:25:31.731650Z",
     "shell.execute_reply.started": "2025-11-01T16:25:31.719122Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class DistillationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid version — identical math, but with safe device handling & small memory optimizations.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_criterion: nn.Module, teacher_model: nn.Module,\n",
    "                 distillation_type: str = 'soft', alpha: float = 0.5, tau: float = 2.0):\n",
    "        super().__init__()\n",
    "        assert distillation_type in ['none', 'soft', 'hard']\n",
    "        self.base_criterion = base_criterion\n",
    "        self.teacher_model = teacher_model\n",
    "        self.distillation_type = distillation_type\n",
    "        self.alpha = float(alpha)\n",
    "        self.tau = float(tau)\n",
    "\n",
    "    def _extract_logits(self, out):\n",
    "        \"\"\"Normalizes many output formats to (main_logits, kd_logits or None).\"\"\"\n",
    "        if isinstance(out, torch.Tensor):\n",
    "            return out, None\n",
    "        if isinstance(out, (tuple, list)):\n",
    "            return out[0], out[1] if len(out) > 1 else None\n",
    "        if isinstance(out, dict):\n",
    "            main = None\n",
    "            kd = None\n",
    "            for k in ['logits', 'out', 'pred', 'logit']:\n",
    "                if k in out:\n",
    "                    main = out[k]\n",
    "                    break\n",
    "            for k in ['kd', 'kd_logits', 'distill', 'distill_logits', 'logits_kd']:\n",
    "                if k in out:\n",
    "                    kd = out[k]\n",
    "                    break\n",
    "            if main is None:\n",
    "                # fallback: pick first tensor in dict\n",
    "                for v in out.values():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        main = v\n",
    "                        break\n",
    "            return main, kd\n",
    "        raise ValueError(\"Unsupported model output type for distillation\")\n",
    "\n",
    "    def forward(self, inputs, outputs, labels):\n",
    "        \"\"\"Compute classification + distillation loss safely.\"\"\"\n",
    "        # Determine device\n",
    "        device = None\n",
    "        try:\n",
    "            params = list(self.base_criterion.parameters())\n",
    "            if len(params) > 0:\n",
    "                device = params[0].device\n",
    "        except Exception:\n",
    "            pass\n",
    "        if device is None:\n",
    "            # Fallback: infer from outputs or labels\n",
    "            if isinstance(outputs, torch.Tensor):\n",
    "                device = outputs.device\n",
    "            elif isinstance(labels, torch.Tensor):\n",
    "                device = labels.device\n",
    "            else:\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Move teacher model to same device if not already\n",
    "        if self.teacher_model is not None:\n",
    "            self.teacher_model.to(device)\n",
    "\n",
    "        # Extract logits\n",
    "        student_logits, student_kd_logits = self._extract_logits(outputs)\n",
    "        student_logits = student_logits.to(device)\n",
    "        labels = labels.to(device)\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        base_loss = self.base_criterion(student_logits, labels)\n",
    "\n",
    "        # If no distillation, return base loss\n",
    "        if self.distillation_type == 'none' or self.teacher_model is None:\n",
    "            return base_loss\n",
    "\n",
    "        # Teacher forward (no grad)\n",
    "        with torch.no_grad():\n",
    "            teacher_out = self.teacher_model(inputs)\n",
    "            teacher_logits, _ = self._extract_logits(teacher_out)\n",
    "            teacher_logits = teacher_logits.to(device)\n",
    "\n",
    "        if student_kd_logits is None:\n",
    "            student_kd_logits = student_logits\n",
    "\n",
    "        # Distillation loss\n",
    "        if self.distillation_type == 'soft':\n",
    "            T = float(self.tau)\n",
    "            s_log_prob = F.log_softmax(student_kd_logits / T, dim=1)\n",
    "            t_prob = F.softmax(teacher_logits / T, dim=1)\n",
    "            distillation_loss = F.kl_div(s_log_prob, t_prob, reduction='batchmean') * (T * T)\n",
    "        else:  # hard\n",
    "            teacher_pred = torch.argmax(teacher_logits, dim=1)\n",
    "            distillation_loss = F.cross_entropy(student_kd_logits, teacher_pred)\n",
    "\n",
    "        # Combine losses\n",
    "        loss = base_loss * (1.0 - self.alpha) + distillation_loss * self.alpha\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:25:45.102452Z",
     "iopub.status.busy": "2025-11-01T16:25:45.101897Z",
     "iopub.status.idle": "2025-11-01T16:25:45.106377Z",
     "shell.execute_reply": "2025-11-01T16:25:45.105549Z",
     "shell.execute_reply.started": "2025-11-01T16:25:45.102430Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "def deepcopy_model_to_cpu(model): \"\"\" Safe version that clones the model's state_dict (not tensors with grad_fn) Works even if model uses weight norm, buffers, or task tokens. \"\"\" # Ensure everything is detached and gradients cleared for p in model.parameters(): if p.grad is not None: p.grad = None p.detach_() torch.cuda.empty_cache() # Create a new instance of same class teacher = model.__class__.__new__(model.__class__) # Shallow copy static attributes teacher.__dict__ = copy.copy(model.__dict__) # Safely copy weights with torch.no_grad(): state_dict = { k: v.detach().cpu().clone() for k, v in model.state_dict().items() } teacher.load_state_dict(state_dict, strict=False) # Freeze params and move to CPU for p in teacher.parameters(): p.requires_grad = False teacher.eval() teacher.cpu() return teacher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:25:50.583470Z",
     "iopub.status.busy": "2025-11-01T16:25:50.583197Z",
     "iopub.status.idle": "2025-11-01T16:25:50.590186Z",
     "shell.execute_reply": "2025-11-01T16:25:50.589310Z",
     "shell.execute_reply.started": "2025-11-01T16:25:50.583451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def evaluate_task(model, loader, device):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        \n",
    "        out = model(imgs)\n",
    "        logits = out[\"logits\"] if isinstance(out, dict) else out\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return 100.0 * correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Compute average accuracy and forgetting\n",
    "# -----------------------------\n",
    "def compute_metrics(acc_matrix):\n",
    "    n_tasks = len(acc_matrix)\n",
    "    final_acc = acc_matrix[-1]\n",
    "    avg_acc = sum(final_acc) / len(final_acc)\n",
    "\n",
    "    forgetting = []\n",
    "    # Compute forgetting for all previous tasks up to current\n",
    "    for i in range(n_tasks - 1):\n",
    "        # accuracy of task i after each later task\n",
    "        acc_i_over_time = [acc_matrix[t][i] for t in range(i, n_tasks)]\n",
    "        max_prev = max(acc_i_over_time[:-1])  # best accuracy before last\n",
    "        final = acc_i_over_time[-1]\n",
    "        forgetting.append(max_prev - final)\n",
    "\n",
    "    avg_forgetting = sum(forgetting) / len(forgetting) if forgetting else 0.0\n",
    "    return avg_acc, avg_forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:26:13.727339Z",
     "iopub.status.busy": "2025-11-01T16:26:13.726759Z",
     "iopub.status.idle": "2025-11-01T16:26:13.737640Z",
     "shell.execute_reply": "2025-11-01T16:26:13.736724Z",
     "shell.execute_reply.started": "2025-11-01T16:26:13.727316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "\n",
    "import torch, random\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity_per_task=200, store_size=(112,112)):\n",
    "        self.capacity_per_task = capacity_per_task\n",
    "        self.store_size = store_size\n",
    "        self.data = []       # list of (C,H,W) tensors (CPU)\n",
    "        self.targets = []    # list of int labels\n",
    "        self.task_ids = []   # list of int task ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def is_empty(self):\n",
    "        return len(self.data) == 0\n",
    "\n",
    "    def add_samples(self, task_id, loader, max_samples=None):\n",
    "        \"\"\"Collect up to max_samples exemplars from the given loader (stored on CPU).\"\"\"\n",
    "        if max_samples is None:\n",
    "            max_samples = self.capacity_per_task\n",
    "        collected = 0\n",
    "        for imgs, labels in loader:\n",
    "            for i in range(imgs.size(0)):\n",
    "                if collected >= max_samples:\n",
    "                    break\n",
    "                img = imgs[i].cpu()\n",
    "                lab = int(labels[i].item())\n",
    "                self.data.append(img)\n",
    "                self.targets.append(lab)\n",
    "                self.task_ids.append(int(task_id))\n",
    "                collected += 1\n",
    "            if collected >= max_samples:\n",
    "                break\n",
    "\n",
    "        # --- Enforce per-task capacity ---\n",
    "        items = list(zip(self.data, self.targets, self.task_ids))\n",
    "        kept = []\n",
    "        for tid in sorted(set(self.task_ids)):\n",
    "            per_task = [x for x in items if x[2] == tid]\n",
    "            if len(per_task) > self.capacity_per_task:\n",
    "                per_task = per_task[-self.capacity_per_task:]\n",
    "            kept.extend(per_task)\n",
    "        self.data = [x[0] for x in kept]\n",
    "        self.targets = [x[1] for x in kept]\n",
    "        self.task_ids = [x[2] for x in kept]\n",
    "\n",
    "    def sample_batch(self, batch_size=16, device=\"cuda\"):\n",
    "        \"\"\"Randomly sample a batch (returns tensors on device).\"\"\"\n",
    "        if len(self.data) == 0:\n",
    "            return None, None, None\n",
    "        bs = min(batch_size, len(self.data))\n",
    "        idx = random.sample(range(len(self.data)), bs)\n",
    "        imgs = torch.stack([self.data[i] for i in idx]).to(device)\n",
    "        labs = torch.tensor([self.targets[i] for i in idx], dtype=torch.long, device=device)\n",
    "        tasks = torch.tensor([self.task_ids[i] for i in idx], dtype=torch.long, device=device)\n",
    "        return imgs, labs, tasks\n",
    "\n",
    "    def get_loader(self, batch_size=32, shuffle=True):\n",
    "        \"\"\"Return a DataLoader over all replay data (on CPU).\"\"\"\n",
    "        if len(self.data) == 0:\n",
    "            return None\n",
    "        imgs = torch.stack(self.data)\n",
    "        labs = torch.tensor(self.targets, dtype=torch.long)\n",
    "        dataset = TensorDataset(imgs, labs)\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:26:50.739864Z",
     "iopub.status.busy": "2025-11-01T16:26:50.739167Z",
     "iopub.status.idle": "2025-11-01T16:26:50.777091Z",
     "shell.execute_reply": "2025-11-01T16:26:50.776395Z",
     "shell.execute_reply.started": "2025-11-01T16:26:50.739825Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === REQUIRED imports ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "from tqdm import tqdm\n",
    "import copy, gc, math\n",
    "\n",
    "# === DEFAULT HYPERPARAMS (good starting point for Kaggle T4) ===\n",
    "CONFIG = {\n",
    "    \"epochs_per_task\": 10,         # main training epochs per task (increase if you want more current-task accuracy)\n",
    "    \"finetune_epochs\": 3,        # balanced finetuning after main train\n",
    "    \"batch_size\": 32,\n",
    "    \"replay_per_task\": 300,      # store-size (on CPU) per task\n",
    "    \"replay_sample_ratio\": 0.25, # fraction of minibatch to replace with replay\n",
    "    \"backbone_lr\": 3e-5,\n",
    "    \"head_lr\": 1e-4,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"unfreeze_last_sabs\": 1,     # number of last shared blocks to unfreeze (ConViT)\n",
    "    \"distill_tau\": 2.0,\n",
    "    \"distill_alpha\": 0.5\n",
    "}\n",
    "\n",
    "# === Helper: build optimizer param groups (backbone vs head) ===\n",
    "def build_optimizer_for_model(model, lr_backbone, lr_head, weight_decay):\n",
    "    backbone_params = []\n",
    "    head_params = []\n",
    "    # Heuristic: params under 'head' name considered head; else backbone\n",
    "    for name, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if \"head\" in name:\n",
    "            head_params.append(p)\n",
    "        else:\n",
    "            backbone_params.append(p)\n",
    "    # fallback: if head empty, attempt to use last head module\n",
    "    if len(head_params) == 0:\n",
    "        try:\n",
    "            if isinstance(model.head, nn.ModuleList):\n",
    "                for p in model.head[-1].parameters():\n",
    "                    if p.requires_grad:\n",
    "                        head_params.append(p)\n",
    "            elif isinstance(model.head, nn.Module):\n",
    "                for p in model.head.parameters():\n",
    "                    if p.requires_grad:\n",
    "                        head_params.append(p)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    param_groups = [\n",
    "        {\"params\": backbone_params, \"lr\": lr_backbone, \"weight_decay\": weight_decay},\n",
    "        {\"params\": head_params, \"lr\": lr_head, \"weight_decay\": weight_decay},\n",
    "    ]\n",
    "    return optim.AdamW(param_groups)\n",
    "\n",
    "# === Balanced finetune dataset wrapper (uses memory + recent task data, balances classes) ===\n",
    "class MemoryBalancedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Expects memory_data: lists (imgs_cpu tensors, label ints, task_id ints)\n",
    "    and new_data: a dataset (torch Dataset) for current task. We'll sample per-class balanced\n",
    "    by simple upsampling/replication to equalize counts.\n",
    "    \"\"\"\n",
    "    def __init__(self, memory_imgs, memory_labels, new_loader, transform=None, oversample_old=1):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        # memory entries already CPU tensors\n",
    "        if memory_imgs is not None and len(memory_imgs) > 0:\n",
    "            self.data.extend(memory_imgs)\n",
    "            self.labels.extend(memory_labels)\n",
    "        # also add current task data (we'll pull images from loader)\n",
    "        new_imgs, new_labels = [], []\n",
    "        for imgs, labs in new_loader:\n",
    "            for i in range(imgs.size(0)):\n",
    "                new_imgs.append(imgs[i].cpu())\n",
    "                new_labels.append(int(labs[i].item()))\n",
    "        self.data.extend(new_imgs)\n",
    "        self.labels.extend(new_labels)\n",
    "        # simple balancing: replicate underrepresented classes up to max_count\n",
    "        counts = {}\n",
    "        for l in self.labels:\n",
    "            counts[l] = counts.get(l, 0) + 1\n",
    "        maxc = max(counts.values()) if counts else 0\n",
    "        balanced_data, balanced_labels = [], []\n",
    "        per_class = {}\n",
    "        for d, l in zip(self.data, self.labels):\n",
    "            per_class.setdefault(l, []).append(d)\n",
    "        for cls, items in per_class.items():\n",
    "            # replicate items to reach maxc\n",
    "            repeat = math.ceil(maxc / max(1, len(items)))\n",
    "            extended = (items * repeat)[:maxc]\n",
    "            balanced_data.extend(extended)\n",
    "            balanced_labels.extend([cls] * len(extended))\n",
    "        self.data, self.labels = balanced_data, balanced_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# === Main training function per-task ===\n",
    "def train_task_with_replay_and_distill(\n",
    "    model,\n",
    "    t_id,\n",
    "    train_loader,\n",
    "    val_loaders,\n",
    "    replay,                 # your replay object with add_samples(task_id, loader, max_samples) and sample_batch(batch_size)-> imgs_cpu, labels_cpu, tasks_cpu (or None for tasks)\n",
    "    old_model_cpu,          # snapshot returned by deepcopy_model_to_cpu from previous iteration (or None)\n",
    "    compute_metrics=None,\n",
    "    cfg=CONFIG,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains model on one new task t_id in-place. Returns snapshot (old_model cpu) for next iteration and accs.\n",
    "    Key assumptions:\n",
    "      - model.add_model(nb_new_classes) exists; DyTox-like model returns dict {'logits':..., 'tokens':...} from forward\n",
    "      - model.set_task(task_id) sets the active head/token for forward routing\n",
    "      - replay has add_samples(task_id, loader, max_samples) and sample_batch(batch_size) -> (imgs_cpu, labs_cpu, tasks_cpu)\n",
    "    \"\"\"\n",
    "    # ---- 1. prepare teacher (old snapshot) and model expansion ----\n",
    "    teacher = None\n",
    "    if old_model_cpu is not None:\n",
    "        teacher = old_model_cpu  # CPU snapshot\n",
    "        # We'll move teacher to device temporarily during forward (no grads)\n",
    "    # For first task: assume model already created with first head; else call add_model appropriately\n",
    "    if t_id == 0:\n",
    "        model.set_task(0)\n",
    "    else:\n",
    "        model.add_model(2)     # CDDB adds 2 classes per task\n",
    "        model.set_task(t_id)\n",
    "\n",
    "        # Freeze old tokens & optionally old heads to protect them\n",
    "        for i in range(len(model.task_tokens) - 1):\n",
    "            model.task_tokens[i].requires_grad = False\n",
    "        if isinstance(model.head, nn.ModuleList):\n",
    "            for head_i in range(len(model.head) - 1):\n",
    "                for p in model.head[head_i].parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "    # ---- selective unfreeze (ConViT specifics) ----\n",
    "    # Freeze everything then selectively unfreeze:\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    # unfreeze last few sabs (shared encoder blocks)\n",
    "    num_unfreeze = cfg[\"unfreeze_last_sabs\"]\n",
    "    if hasattr(model, \"sabs\") and len(model.sabs) > 0:\n",
    "        for blk in model.sabs[-num_unfreeze:]:\n",
    "            for p in blk.parameters():\n",
    "                p.requires_grad = True\n",
    "    # unfreeze TABs (task attention blocks)\n",
    "    if hasattr(model, \"tabs\"):\n",
    "        for blk in model.tabs:\n",
    "            for p in blk.parameters():\n",
    "                p.requires_grad = True\n",
    "    # unfreeze all head params (especially new head)\n",
    "    if isinstance(model.head, nn.ModuleList):\n",
    "        for head in model.head:\n",
    "            for p in head.parameters():\n",
    "                p.requires_grad = True\n",
    "    else:\n",
    "        for p in model.head.parameters():\n",
    "            p.requires_grad = True\n",
    "    # unfreeze last task token(s)\n",
    "    for p in model.task_tokens[-1:]:\n",
    "        p.requires_grad = True\n",
    "\n",
    "    # ---- criterion (base + distillation if teacher exists) ----\n",
    "    base_loss = LabelSmoothingCrossEntropyBoosting(smoothing=0.1).to(device)\n",
    "    if teacher is not None:\n",
    "        criterion = DistillationLoss(base_criterion=base_loss, teacher_model=None,\n",
    "                                     distillation_type='soft',\n",
    "                                     alpha=cfg[\"distill_alpha\"], tau=cfg[\"distill_tau\"])\n",
    "        # We'll set teacher reference per-batch (move to device for forward only)\n",
    "    else:\n",
    "        criterion = base_loss\n",
    "\n",
    "    # ---- optimizer AFTER deciding requires_grad ----\n",
    "    optimizer = build_optimizer_for_model(model, cfg[\"backbone_lr\"], cfg[\"head_lr\"], cfg[\"weight_decay\"])\n",
    "    scaler = GradScaler()\n",
    "    model.to(device).float()\n",
    "    model.train()\n",
    "\n",
    "    # ---- training epochs (main loop) ------------------------------------\n",
    "    for epoch in range(cfg[\"epochs_per_task\"]):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Task {t_id} | Epoch {epoch+1}/{cfg['epochs_per_task']}\")\n",
    "        for imgs, labels in pbar:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # ensure model is routing to current task for current batch\n",
    "            model.set_task(t_id)\n",
    "\n",
    "            # --- forward current batch ---\n",
    "            with autocast():\n",
    "                outputs = model(imgs)\n",
    "                if isinstance(criterion, DistillationLoss):\n",
    "                    # if teacher exists, run teacher forward on CPU -> move to device only for logits\n",
    "                    if teacher is not None:\n",
    "                        # teacher is CPU; move to device temporarily for fast forward\n",
    "                       if teacher is not None:\n",
    "                        teacher_dev = copy.deepcopy(model).to(device)\n",
    "                        teacher_dev.load_state_dict(teacher.state_dict())\n",
    "                        teacher_dev.eval()\n",
    "                        with torch.no_grad():\n",
    "                            teacher_out = teacher_dev(imgs)\n",
    "                        # free teacher_dev promptly\n",
    "                        del teacher_dev\n",
    "                        torch.cuda.empty_cache()\n",
    "                        loss_current = criterion(imgs, outputs, labels)  # criterion will do teacher forward if you set teacher_model\n",
    "                        # To be safe, set teacher inside criterion here:\n",
    "                        criterion.teacher_model = None  # ensure criterion not using old internal model\n",
    "                        # We will compute distillation loss manually below using teacher_out to avoid double forward\n",
    "                        # Use criterion logic: extract student logits, teacher logits and compute\n",
    "                        # Simpler: call a helper distill computation (we'll do a simple soft KL here)\n",
    "                        student_logits, student_kd = criterion._extract_logits(outputs)\n",
    "                        teacher_logits, _ = criterion._extract_logits(teacher_out)\n",
    "                        if student_kd is None:\n",
    "                            student_kd = student_logits\n",
    "                        T = float(criterion.tau)\n",
    "                        s_log_prob = F.log_softmax(student_kd / T, dim=1)\n",
    "                        t_prob = F.softmax(teacher_logits / T, dim=1)\n",
    "                        distill_loss = F.kl_div(s_log_prob, t_prob, reduction='batchmean') * (T * T)\n",
    "                        base_l = base_loss(student_logits, labels)\n",
    "                        loss_current = base_l * (1.0 - criterion.alpha) + distill_loss * criterion.alpha\n",
    "                    else:\n",
    "                        # no teacher -> just base loss via the wrapper\n",
    "                        loss_current = criterion(imgs, outputs, labels)\n",
    "                else:\n",
    "                    logits = outputs[\"logits\"] if isinstance(outputs, dict) else outputs\n",
    "                    loss_current = criterion(logits, labels)\n",
    "\n",
    "            scaler.scale(loss_current).backward()\n",
    "\n",
    "            # --- replay step: sample a small replay batch and backprop separately per-task ---\n",
    "            if t_id > 0 and len(replay.data) > 0:\n",
    "                # number from config: either ratio*batch or at least 1\n",
    "                replay_n = max(1, int(imgs.size(0) * cfg[\"replay_sample_ratio\"]))\n",
    "                r_imgs_cpu, r_labels_cpu, r_tasks_cpu = replay.sample_batch(replay_n)  # returns CPU tensors + task ids\n",
    "                if r_imgs_cpu is not None:\n",
    "                    # group by tasks to route to correct head\n",
    "                    tasks_list = r_tasks_cpu.tolist() if r_tasks_cpu is not None else [0]*r_imgs_cpu.size(0)\n",
    "                    # move to device and resize\n",
    "                    r_imgs_full = F.interpolate(r_imgs_cpu.to(device), size=(224,224))\n",
    "                    r_labels_full = r_labels_cpu.to(device)\n",
    "                    # per-task grouping\n",
    "                    for task_idx in sorted(set(tasks_list)):\n",
    "                        idxs = [i for i, t in enumerate(tasks_list) if t == task_idx]\n",
    "                        if len(idxs) == 0:\n",
    "                            continue\n",
    "                        sel_imgs = r_imgs_full[idxs]\n",
    "                        sel_labels = r_labels_full[idxs]\n",
    "                        model.set_task(int(task_idx))\n",
    "                        with autocast():\n",
    "                            out_replay = model(sel_imgs)\n",
    "                            if isinstance(criterion, DistillationLoss) and teacher is not None:\n",
    "                                # compute distill using teacher snapshot for those replay inputs\n",
    "                                teacher_dev = copy.deepcopy(teacher).to(device)\n",
    "                                teacher_dev.eval()\n",
    "                                with torch.no_grad():\n",
    "                                    teacher_out_replay = teacher_dev(sel_imgs)\n",
    "                                # compute same as above:\n",
    "                                student_logits_r, student_kd_r = criterion._extract_logits(out_replay)\n",
    "                                teacher_logits_r, _ = criterion._extract_logits(teacher_out_replay)\n",
    "                                if student_kd_r is None:\n",
    "                                    student_kd_r = student_logits_r\n",
    "                                T = float(criterion.tau)\n",
    "                                s_log_prob = F.log_softmax(student_kd_r / T, dim=1)\n",
    "                                t_prob = F.softmax(teacher_logits_r / T, dim=1)\n",
    "                                distill_loss_r = F.kl_div(s_log_prob, t_prob, reduction='batchmean') * (T * T)\n",
    "                                base_r = base_loss(student_logits_r, sel_labels)\n",
    "                                loss_replay = base_r * (1.0 - criterion.alpha) + distill_loss_r * criterion.alpha\n",
    "                                del teacher_dev; torch.cuda.empty_cache()\n",
    "                            else:\n",
    "                                logits_replay = out_replay[\"logits\"] if isinstance(out_replay, dict) else out_replay\n",
    "                                loss_replay = base_loss(logits_replay, sel_labels)\n",
    "                        scaler.scale(loss_replay).backward()\n",
    "\n",
    "            # ---- finalize step: unscale, clip, detect inf/nan grads, step ----\n",
    "            scaler.unscale_(optimizer)\n",
    "            # detect non-finite grads\n",
    "            grads_ok = True\n",
    "            for group in optimizer.param_groups:\n",
    "                for p in group[\"params\"]:\n",
    "                    if p.grad is None:\n",
    "                        continue\n",
    "                    if not torch.isfinite(p.grad).all():\n",
    "                        grads_ok = False\n",
    "                        break\n",
    "                if not grads_ok:\n",
    "                    break\n",
    "            if not grads_ok:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scaler.update()\n",
    "                # skip step to avoid corrupting weights\n",
    "                continue\n",
    "\n",
    "            # clip and step\n",
    "            torch.nn.utils.clip_grad_norm_([p for g in optimizer.param_groups for p in g[\"params\"] if p.grad is not None],\n",
    "                                           cfg[\"grad_clip\"])\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # bookkeeping\n",
    "            running_loss += loss_current.item() * imgs.size(0)\n",
    "            logits_out = outputs[\"logits\"] if isinstance(outputs, dict) else outputs\n",
    "            preds = torch.argmax(logits_out, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            pbar.set_postfix({\"Acc\": f\"{100*correct/total:.2f}%\", \"Loss\": f\"{running_loss/(total+1e-9):.4f}\"})\n",
    "\n",
    "        # epoch end\n",
    "        epoch_loss = running_loss / max(1, total)\n",
    "        epoch_acc = 100.0 * correct / max(1, total)\n",
    "        print(f\"Task {t_id} | Epoch {epoch+1}/{cfg['epochs_per_task']} | Loss={epoch_loss:.4f} | Acc={epoch_acc:.2f}%\")\n",
    "\n",
    "    # ---- Balanced finetuning using memory + current data (helps both current accuracy and reduce forgetting) ----\n",
    "    # Build balanced dataset\n",
    "    memory_imgs = replay.data if hasattr(replay, \"data\") else None\n",
    "    memory_labels = replay.targets if hasattr(replay, \"targets\") else None\n",
    "    finetune_dataset = MemoryBalancedDataset(memory_imgs, memory_labels, train_loader)\n",
    "    if len(finetune_dataset) > 0:\n",
    "        ft_loader = DataLoader(finetune_dataset, batch_size=cfg[\"batch_size\"], shuffle=True)\n",
    "        # fine-tune only heads + last shared block (lower LR)\n",
    "        # freeze everything, then unfreeze head & last sabs and tabs\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False\n",
    "        if hasattr(model, \"sabs\") and len(model.sabs) > 0:\n",
    "            for p in model.sabs[-cfg[\"unfreeze_last_sabs\"]:].parameters():\n",
    "                p.requires_grad = True\n",
    "        if hasattr(model, \"tabs\"):\n",
    "            for p in model.tabs.parameters():\n",
    "                p.requires_grad = True\n",
    "        # unfreeze heads\n",
    "        if isinstance(model.head, nn.ModuleList):\n",
    "            for p in model.head[-1].parameters():  # prefer new head\n",
    "                p.requires_grad = True\n",
    "        else:\n",
    "            for p in model.head.parameters():\n",
    "                p.requires_grad = True\n",
    "        # optimizer for finetune (head-only + last block)\n",
    "        ft_optimizer = build_optimizer_for_model(model, lr_backbone=cfg[\"backbone_lr\"]/10, lr_head=cfg[\"head_lr\"], weight_decay=cfg[\"weight_decay\"])\n",
    "        ft_scaler = GradScaler()\n",
    "        model.train()\n",
    "        for _ in range(cfg[\"finetune_epochs\"]):\n",
    "            for imgs_ft, labels_ft in ft_loader:\n",
    "                imgs_ft = imgs_ft.to(device)\n",
    "                labels_ft = labels_ft.to(device)\n",
    "                ft_optimizer.zero_grad(set_to_none=True)\n",
    "                model.set_task(t_id)\n",
    "                with autocast():\n",
    "                    out_ft = model(imgs_ft)\n",
    "                    logits_ft = out_ft[\"logits\"] if isinstance(out_ft, dict) else out_ft\n",
    "                    loss_ft = base_loss(logits_ft, labels_ft)\n",
    "                ft_scaler.scale(loss_ft).backward()\n",
    "                ft_scaler.unscale_(ft_optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_([p for g in ft_optimizer.param_groups for p in g[\"params\"] if p.grad is not None], cfg[\"grad_clip\"])\n",
    "                ft_scaler.step(ft_optimizer)\n",
    "                ft_scaler.update()\n",
    "\n",
    "    # ---- update replay memory with new task examples (on CPU) ----\n",
    "    # prefer using replay.add_samples if it exists\n",
    "    try:\n",
    "        replay.add_samples(t_id, train_loader, max_samples=cfg[\"replay_per_task\"])\n",
    "    except Exception:\n",
    "        # fallback: collect from loader manually\n",
    "        imgs_accum, labs_accum = [], []\n",
    "        for imgs_b, labs_b in train_loader:\n",
    "            imgs_accum.append(imgs_b.cpu())\n",
    "            labs_accum.append(labs_b.cpu())\n",
    "        if len(imgs_accum) > 0:\n",
    "            imgs_all = torch.cat(imgs_accum, dim=0)\n",
    "            labs_all = torch.cat(labs_accum, dim=0)\n",
    "            idxs = torch.randperm(len(labs_all))[:cfg[\"replay_per_task\"]]\n",
    "            replay.data = [imgs_all[i] for i in idxs.tolist()]\n",
    "            replay.targets = [int(l) for l in labs_all[idxs].tolist()]\n",
    "            replay.task_ids = [t_id] * len(idxs)\n",
    "\n",
    "    # ---- snapshot model to CPU for next task teacher (deep copy) ----\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "    with torch.no_grad():\n",
    "            old_model_next = deepcopy_model_to_cpu(model)\n",
    "    accs = []\n",
    "    model.to(device)\n",
    "    for vid in range(t_id + 1):\n",
    "        model.set_task(vid)\n",
    "        acc = evaluate_task(model, val_loaders[vid], device)\n",
    "        accs.append(acc)\n",
    "    return old_model_next, accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:27:14.597936Z",
     "iopub.status.busy": "2025-11-01T16:27:14.597334Z",
     "iopub.status.idle": "2025-11-01T19:00:20.647955Z",
     "shell.execute_reply": "2025-11-01T19:00:20.647036Z",
     "shell.execute_reply.started": "2025-11-01T16:27:14.597911Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_160/1384571469.py:182: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training on Task 0 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 1/10:   0%|          | 0/188 [00:00<?, ?it/s]/tmp/ipykernel_160/1384571469.py:202: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Task 0 | Epoch 1/10: 100%|██████████| 188/188 [02:01<00:00,  1.54it/s, Acc=68.51%, Loss=0.6089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 1/10 | Loss=0.6089 | Acc=68.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 2/10: 100%|██████████| 188/188 [01:41<00:00,  1.85it/s, Acc=76.40%, Loss=0.5375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 2/10 | Loss=0.5375 | Acc=76.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 3/10: 100%|██████████| 188/188 [01:41<00:00,  1.86it/s, Acc=78.62%, Loss=0.5056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 3/10 | Loss=0.5056 | Acc=78.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 4/10: 100%|██████████| 188/188 [01:40<00:00,  1.86it/s, Acc=80.75%, Loss=0.4875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 4/10 | Loss=0.4875 | Acc=80.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 5/10: 100%|██████████| 188/188 [01:40<00:00,  1.87it/s, Acc=83.78%, Loss=0.4459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 5/10 | Loss=0.4459 | Acc=83.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 6/10: 100%|██████████| 188/188 [01:40<00:00,  1.86it/s, Acc=85.82%, Loss=0.4149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 6/10 | Loss=0.4149 | Acc=85.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 7/10: 100%|██████████| 188/188 [01:41<00:00,  1.85it/s, Acc=87.38%, Loss=0.3989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 7/10 | Loss=0.3989 | Acc=87.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 8/10: 100%|██████████| 188/188 [01:41<00:00,  1.85it/s, Acc=88.63%, Loss=0.3781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 8/10 | Loss=0.3781 | Acc=88.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 9/10: 100%|██████████| 188/188 [01:40<00:00,  1.87it/s, Acc=90.10%, Loss=0.3550]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 9/10 | Loss=0.3550 | Acc=90.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 10/10: 100%|██████████| 188/188 [01:41<00:00,  1.85it/s, Acc=90.87%, Loss=0.3468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 10/10 | Loss=0.3468 | Acc=90.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_160/1384571469.py:350: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  ft_scaler = GradScaler()\n",
      "/tmp/ipykernel_160/1384571469.py:358: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 accuracies (all seen tasks): [93.5]\n",
      "\n",
      "===== Training on Task 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]/tmp/ipykernel_160/1384571469.py:261: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Task 1 | Epoch 1/10: 100%|██████████| 75/75 [00:55<00:00,  1.35it/s, Acc=68.14%, Loss=0.6155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 1/10 | Loss=0.6155 | Acc=68.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 2/10: 100%|██████████| 75/75 [00:47<00:00,  1.59it/s, Acc=74.25%, Loss=0.5484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 2/10 | Loss=0.5484 | Acc=74.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 3/10: 100%|██████████| 75/75 [00:47<00:00,  1.59it/s, Acc=79.77%, Loss=0.4916]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 3/10 | Loss=0.4916 | Acc=79.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 4/10: 100%|██████████| 75/75 [00:47<00:00,  1.59it/s, Acc=83.96%, Loss=0.4396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 4/10 | Loss=0.4396 | Acc=83.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 5/10: 100%|██████████| 75/75 [00:47<00:00,  1.59it/s, Acc=85.17%, Loss=0.4196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 5/10 | Loss=0.4196 | Acc=85.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 6/10: 100%|██████████| 75/75 [00:47<00:00,  1.59it/s, Acc=87.08%, Loss=0.3975]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 6/10 | Loss=0.3975 | Acc=87.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 7/10: 100%|██████████| 75/75 [00:47<00:00,  1.59it/s, Acc=88.64%, Loss=0.3723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 7/10 | Loss=0.3723 | Acc=88.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 8/10: 100%|██████████| 75/75 [00:47<00:00,  1.58it/s, Acc=89.58%, Loss=0.3651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 8/10 | Loss=0.3651 | Acc=89.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 9/10: 100%|██████████| 75/75 [00:47<00:00,  1.59it/s, Acc=90.29%, Loss=0.3541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 9/10 | Loss=0.3541 | Acc=90.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 10/10: 100%|██████████| 75/75 [00:47<00:00,  1.59it/s, Acc=93.08%, Loss=0.3269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 10/10 | Loss=0.3269 | Acc=93.08%\n",
      "Task 1 accuracies (all seen tasks): [84.7, 90.75]\n",
      "\n",
      "===== Training on Task 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 1/10: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, Acc=66.22%, Loss=0.6160]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 1/10 | Loss=0.6160 | Acc=66.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 2/10: 100%|██████████| 50/50 [00:34<00:00,  1.43it/s, Acc=83.25%, Loss=0.4705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 2/10 | Loss=0.4705 | Acc=83.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 3/10: 100%|██████████| 50/50 [00:34<00:00,  1.43it/s, Acc=88.93%, Loss=0.3844]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 3/10 | Loss=0.3844 | Acc=88.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 4/10: 100%|██████████| 50/50 [00:35<00:00,  1.43it/s, Acc=90.27%, Loss=0.3656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 4/10 | Loss=0.3656 | Acc=90.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 5/10: 100%|██████████| 50/50 [00:34<00:00,  1.43it/s, Acc=89.76%, Loss=0.3546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 5/10 | Loss=0.3546 | Acc=89.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 6/10: 100%|██████████| 50/50 [00:34<00:00,  1.43it/s, Acc=91.73%, Loss=0.3392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 6/10 | Loss=0.3392 | Acc=91.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 7/10: 100%|██████████| 50/50 [00:34<00:00,  1.44it/s, Acc=93.45%, Loss=0.3151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 7/10 | Loss=0.3151 | Acc=93.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 8/10: 100%|██████████| 50/50 [00:35<00:00,  1.42it/s, Acc=94.21%, Loss=0.2977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 8/10 | Loss=0.2977 | Acc=94.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 9/10: 100%|██████████| 50/50 [00:34<00:00,  1.44it/s, Acc=94.02%, Loss=0.2978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 9/10 | Loss=0.2978 | Acc=94.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 10/10: 100%|██████████| 50/50 [00:35<00:00,  1.42it/s, Acc=94.78%, Loss=0.2834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 10/10 | Loss=0.2834 | Acc=94.78%\n",
      "Task 2 accuracies (all seen tasks): [88.4, 91.875, 91.22137404580153]\n",
      "\n",
      "===== Training on Task 3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 1/10: 100%|██████████| 240/240 [04:02<00:00,  1.01s/it, Acc=97.67%, Loss=0.2397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 1/10 | Loss=0.2397 | Acc=97.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 2/10: 100%|██████████| 240/240 [03:30<00:00,  1.14it/s, Acc=99.44%, Loss=0.2102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 2/10 | Loss=0.2102 | Acc=99.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 3/10: 100%|██████████| 240/240 [03:29<00:00,  1.15it/s, Acc=99.66%, Loss=0.2047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 3/10 | Loss=0.2047 | Acc=99.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 4/10: 100%|██████████| 240/240 [03:30<00:00,  1.14it/s, Acc=99.75%, Loss=0.2036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 4/10 | Loss=0.2036 | Acc=99.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 5/10: 100%|██████████| 240/240 [03:29<00:00,  1.15it/s, Acc=99.84%, Loss=0.2016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 5/10 | Loss=0.2016 | Acc=99.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 6/10: 100%|██████████| 240/240 [03:29<00:00,  1.15it/s, Acc=99.82%, Loss=0.2022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 6/10 | Loss=0.2022 | Acc=99.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 7/10: 100%|██████████| 240/240 [03:29<00:00,  1.15it/s, Acc=99.79%, Loss=0.2032]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 7/10 | Loss=0.2032 | Acc=99.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 8/10: 100%|██████████| 240/240 [03:28<00:00,  1.15it/s, Acc=99.95%, Loss=0.1997]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 8/10 | Loss=0.1997 | Acc=99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 9/10: 100%|██████████| 240/240 [03:29<00:00,  1.15it/s, Acc=99.88%, Loss=0.2005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 9/10 | Loss=0.2005 | Acc=99.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 10/10: 100%|██████████| 240/240 [03:29<00:00,  1.15it/s, Acc=99.88%, Loss=0.2007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 10/10 | Loss=0.2007 | Acc=99.88%\n",
      "Task 3 accuracies (all seen tasks): [73.75, 73.0, 86.45038167938931, 99.96081504702194]\n",
      "\n",
      "===== Training on Task 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 4 | Epoch 1/10: 100%|██████████| 102/102 [01:51<00:00,  1.09s/it, Acc=68.26%, Loss=0.6026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 4 | Epoch 1/10 | Loss=0.6026 | Acc=68.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 4 | Epoch 2/10: 100%|██████████| 102/102 [01:32<00:00,  1.10it/s, Acc=86.07%, Loss=0.4092]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 4 | Epoch 2/10 | Loss=0.4092 | Acc=86.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 4 | Epoch 3/10: 100%|██████████| 102/102 [01:33<00:00,  1.10it/s, Acc=91.47%, Loss=0.3401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 4 | Epoch 3/10 | Loss=0.3401 | Acc=91.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 4 | Epoch 4/10: 100%|██████████| 102/102 [01:32<00:00,  1.10it/s, Acc=93.32%, Loss=0.3018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 4 | Epoch 4/10 | Loss=0.3018 | Acc=93.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 4 | Epoch 5/10: 100%|██████████| 102/102 [01:32<00:00,  1.11it/s, Acc=94.46%, Loss=0.2901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 4 | Epoch 5/10 | Loss=0.2901 | Acc=94.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 4 | Epoch 6/10: 100%|██████████| 102/102 [01:32<00:00,  1.10it/s, Acc=94.77%, Loss=0.2775]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 4 | Epoch 6/10 | Loss=0.2775 | Acc=94.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 4 | Epoch 7/10: 100%|██████████| 102/102 [01:32<00:00,  1.10it/s, Acc=94.98%, Loss=0.2815]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 4 | Epoch 7/10 | Loss=0.2815 | Acc=94.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 4 | Epoch 8/10: 100%|██████████| 102/102 [01:33<00:00,  1.10it/s, Acc=95.91%, Loss=0.2656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 4 | Epoch 8/10 | Loss=0.2656 | Acc=95.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 4 | Epoch 9/10: 100%|██████████| 102/102 [01:33<00:00,  1.09it/s, Acc=95.84%, Loss=0.2690]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 4 | Epoch 9/10 | Loss=0.2690 | Acc=95.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 4 | Epoch 10/10: 100%|██████████| 102/102 [01:34<00:00,  1.08it/s, Acc=96.34%, Loss=0.2593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 4 | Epoch 10/10 | Loss=0.2593 | Acc=96.34%\n",
      "Task 4 accuracies (all seen tasks): [80.7, 86.5, 91.22137404580153, 100.0, 95.10166358595194]\n",
      "\n",
      "===== Training on Task 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 5 | Epoch 1/10: 100%|██████████| 240/240 [05:06<00:00,  1.28s/it, Acc=99.26%, Loss=0.2092]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 5 | Epoch 1/10 | Loss=0.2092 | Acc=99.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 5 | Epoch 2/10: 100%|██████████| 240/240 [04:20<00:00,  1.08s/it, Acc=99.97%, Loss=0.1989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 5 | Epoch 2/10 | Loss=0.1989 | Acc=99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 5 | Epoch 3/10: 100%|██████████| 240/240 [04:20<00:00,  1.09s/it, Acc=99.97%, Loss=0.1991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 5 | Epoch 3/10 | Loss=0.1991 | Acc=99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 5 | Epoch 4/10: 100%|██████████| 240/240 [04:20<00:00,  1.08s/it, Acc=99.92%, Loss=0.2000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 5 | Epoch 4/10 | Loss=0.2000 | Acc=99.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 5 | Epoch 5/10: 100%|██████████| 240/240 [04:19<00:00,  1.08s/it, Acc=99.92%, Loss=0.2001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 5 | Epoch 5/10 | Loss=0.2001 | Acc=99.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 5 | Epoch 6/10: 100%|██████████| 240/240 [04:20<00:00,  1.09s/it, Acc=99.99%, Loss=0.1988] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 5 | Epoch 6/10 | Loss=0.1988 | Acc=99.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 5 | Epoch 7/10: 100%|██████████| 240/240 [04:21<00:00,  1.09s/it, Acc=99.99%, Loss=0.1989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 5 | Epoch 7/10 | Loss=0.1989 | Acc=99.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 5 | Epoch 8/10: 100%|██████████| 240/240 [04:18<00:00,  1.08s/it, Acc=100.00%, Loss=0.1985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 5 | Epoch 8/10 | Loss=0.1985 | Acc=100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 5 | Epoch 9/10: 100%|██████████| 240/240 [04:20<00:00,  1.08s/it, Acc=100.00%, Loss=0.1985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 5 | Epoch 9/10 | Loss=0.1985 | Acc=100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 5 | Epoch 10/10: 100%|██████████| 240/240 [04:20<00:00,  1.09s/it, Acc=100.00%, Loss=0.1985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 5 | Epoch 10/10 | Loss=0.1985 | Acc=100.00%\n",
      "Task 5 accuracies (all seen tasks): [76.45, 76.5, 89.8854961832061, 99.92163009404389, 92.51386321626617, 99.92163009404389]\n"
     ]
    }
   ],
   "source": [
    "replay = ReplayBuffer(capacity_per_task=CONFIG[\"replay_per_task\"])\n",
    "\n",
    "# datasets per task (example)\n",
    "# train_loaders = [train_loader_task0, train_loader_task1, train_loader_task2, ...]\n",
    "# val_loaders =   [val_loader_task0,   val_loader_task1,   val_loader_task2,   ...]\n",
    "\n",
    "# for the first iteration, there’s no teacher model yet\n",
    "old_model_cpu = None\n",
    "\n",
    "# to record accuracy history\n",
    "taskwise_accs = []\n",
    "# Loop through each incremental task\n",
    "for t_id in range(len(task_loaders)):\n",
    "    print(f\"\\n===== Training on Task {t_id} =====\")\n",
    "\n",
    "    # call the training method\n",
    "    old_model_cpu, accs = train_task_with_replay_and_distill(\n",
    "        model=model,\n",
    "        t_id=t_id,\n",
    "        train_loader=task_loaders[t_id],\n",
    "        val_loaders=val_loaders,\n",
    "        replay=replay,\n",
    "        old_model_cpu=old_model_cpu,\n",
    "        compute_metrics=None,   # optional\n",
    "        cfg=CONFIG,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    print(f\"Task {t_id} accuracies (all seen tasks): {accs}\")\n",
    "    taskwise_accs.append(accs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T19:01:09.784313Z",
     "iopub.status.busy": "2025-11-01T19:01:09.784039Z",
     "iopub.status.idle": "2025-11-01T19:01:09.789039Z",
     "shell.execute_reply": "2025-11-01T19:01:09.788161Z",
     "shell.execute_reply.started": "2025-11-01T19:01:09.784292Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final per-task accuracies:\n",
      "After Task 0: [93.5]\n",
      "After Task 1: [84.7, 90.75]\n",
      "After Task 2: [88.4, 91.875, 91.22137404580153]\n",
      "After Task 3: [73.75, 73.0, 86.45038167938931, 99.96081504702194]\n",
      "After Task 4: [80.7, 86.5, 91.22137404580153, 100.0, 95.10166358595194]\n",
      "After Task 5: [76.45, 76.5, 89.8854961832061, 99.92163009404389, 92.51386321626617, 99.92163009404389]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final per-task accuracies:\")\n",
    "for i, accs in enumerate(taskwise_accs):\n",
    "    print(f\"After Task {i}: {accs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T19:05:43.414215Z",
     "iopub.status.busy": "2025-11-01T19:05:43.413536Z",
     "iopub.status.idle": "2025-11-01T19:05:43.418458Z",
     "shell.execute_reply": "2025-11-01T19:05:43.417631Z",
     "shell.execute_reply.started": "2025-11-01T19:05:43.414189Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Accuracy for last task: 89.20%\n"
     ]
    }
   ],
   "source": [
    "last_list = taskwise_accs[-1]  # get the most recent task's accuracy list\n",
    "avg_last = sum(last_list) / len(last_list)\n",
    "print(f\"\\nAverage Accuracy for last task: {avg_last:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T19:13:06.140957Z",
     "iopub.status.busy": "2025-11-01T19:13:06.140268Z",
     "iopub.status.idle": "2025-11-01T19:13:06.590927Z",
     "shell.execute_reply": "2025-11-01T19:13:06.590029Z",
     "shell.execute_reply.started": "2025-11-01T19:13:06.140928Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and replay buffer saved successfully!\n",
      "➡ You can now download them from the 'Data' > 'Output' section in Kaggle.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "save_dir = \"/kaggle/working/checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(save_dir, \"final_model.pth\")\n",
    "replay_path = os.path.join(save_dir, \"replay_buffer.pkl\")\n",
    "\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "\n",
    "\n",
    "print(\"✅ Model and replay buffer saved successfully!\")\n",
    "print(\"➡ You can now download them from the 'Data' > 'Output' section in Kaggle.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T19:13:46.765506Z",
     "iopub.status.busy": "2025-11-01T19:13:46.764690Z",
     "iopub.status.idle": "2025-11-01T19:13:46.771298Z",
     "shell.execute_reply": "2025-11-01T19:13:46.770441Z",
     "shell.execute_reply.started": "2025-11-01T19:13:46.765473Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='./checkpoints/final_model.pth' target='_blank'>./checkpoints/final_model.pth</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/checkpoints/final_model.pth"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# Make downloadable links\n",
    "FileLink(\"./checkpoints/final_model.pth\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8616387,
     "sourceId": 13564591,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
